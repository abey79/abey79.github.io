[{"content":"vpype 1.13 is out! üéâ\nThe main improvement is Python 3.11 support, but it does come with a few other bells and whistles, so let\u0026rsquo;s dive in.\nPython 3.11 support  Added support for Python 3.11 and dropped support for Python 3.8 (#581)  Known issue:\n As of PySide 6.4.2, a refresh issue arises on macOS when the viewer window is resized by a window manager (#603)   Python 3.11 was first released on October 22nd, 2022, and the current version is 3.11.2 as of this writing. So it was way overdue for vpype to support it.\nAs usual, adding support for a newer Python means dropping an older one\u0026mdash;bye bye Python 3.8! üëã That said, if you must use Python 3.8, vpype 1.12.1 remains available and has been a rather stable release.\nIn the process of adapting vpype for 3.11, I had to update PySide6 to the last release (6.4.2 as of this writing). Unfortunately, it introduces a refresh glitch on macOS when using a window manager:\nYour browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!   I waited for a fix, but this is taking too long, so I opted to make a release with this known issue.\nlid built-in expression variable  Added the lid built-in expression variable for generator and layer processor commands (#605)   Many vpype commands operate on a layer-by-layer basis. They are known as layer processors. One example is the translate command. By default, it appears to run on all layers at once, but what actually happens is that it is run once for each layer. You can control on which layer(s) it should actually run with the --layer option.\nAny expression passed to such a command is also evaluated once per layer. This enables per-layer customisation of the command behaviour. This feature introduces the lid built-in variable, which is set to the ID of the layer that\u0026rsquo;s being currently processed.\nHere is an example for illustration:\n$ vpype read file_with_many_layers.svg name \u0026#34;Layer %lid%\u0026#34; write output.svg Here, the name command renames all layers (--layer is not used). But since lid is set by vpype to the current layer ID, the output SVG has its layers labelled \u0026ldquo;Layer 1\u0026rdquo;, \u0026ldquo;Layer 2\u0026rdquo;, etc. (the exact numbers depend on the input SVG\u0026rsquo;s actual layer structure).\nLayer ID determination  Fixed a design issue with the read command where disjoint groups of digits in layer names could be used to determine layer IDs. Only the first contiguous group of digits is used now, so a layer named \u0026ldquo;01-layer1\u0026rdquo; has layer ID of 1 instead of 11 (#606)   Speaking of layer IDs, this fix slightly changes how vpype attributes layer IDs when reading SVGs using the read command. Previously, vpype would strip all non-digit characters from the layer name (or, lacking one, from the corresponding group ID), and use the resulting number as ID, if any. A layer named \u0026ldquo;01-blue3\u0026rdquo; would thus end up with ID 13, which is a rather unexpected result! The new behaviour consists of taking the first contiguous group of digit, if any, and use this as layer ID instead. Now, layer \u0026ldquo;01-blue3\u0026rdquo; has an ID of 1.\nBTW, I\u0026rsquo;m working on a new article on layer names, and how they can be used to control the plotting process with the AxiDraw. So stay tuned for more on this!\nWayland-related crash  Fixed an issue on Wayland-based Linux distributions where using the viewer (e.g. with the show command) would crash (#607)   Qt, the GUI library behind PySide6, has an abstraction layer dedicated to the platform its running on. This is basically what enables Qt projects to seamlessly run on macOS, Linux, or Windows. There appears to be some technicalities involved with this when using OpenGL (as vpype does), which led the viewer to crash on Wayland-based Linux install. The workaround consisted of \u0026ldquo;forcing\u0026rdquo; Qt to use the xcb instead of wayland platform by setting an environment variable:\nexport QT_QPA_PLATFORM=xcb With vpype 1.13, this is no longer needed as it automatically sets this environment variable when a Wayland-based Linux is detected.\n","permalink":"https://bylr.info/articles/2023/03/13/annotated-release-notes-vpype-1.13/","summary":"\u003cp\u003e\u003cem\u003evpype\u003c/em\u003e 1.13 is out! üéâ\u003c/p\u003e\n\u003cp\u003eThe main improvement is Python 3.11 support, but it does come with a few other bells and whistles, so let\u0026rsquo;s dive in.\u003c/p\u003e","title":"Annotated Release Notes: vpype 1.13"},{"content":"What\u0026rsquo;s this? I recently built an \u0026ldquo;Automatic #plotloop Machine\u0026rdquo;, named after the common social media hashtag for these animations made of individually plotted frames. This article is a thorough description of the project, covering both hardware and software aspects.\nFirst, here is a video:\n   So, this is all very nice, but why does this article need to be more than six thousand five hundred word long?! Sure, the machine\u0026rsquo;s neat, but it\u0026rsquo;s not like everyone wants to make one, right?\nWell, in the making of this project, I found myself using a number of tools and techniques which, I think, might be of more general interest\u0026mdash;that is, also for someone without aspirations for maximally complex animation production methods.\nRather than the seasoned maker, who might more efficiently fill knowledge gaps with a few targeted Google queries, I wrote this article for the many people in the plotter space with an artistic rather than technical background. I\u0026rsquo;m hoping this article might provide some technical baggage to fuel their tinkering urge, which, being involved with plotters, they obviously have.\nSo, if any or all of the following topics sound like they might be useful, have a read!\n How to use a Raspberry Pi to automate things like taking pictures or controlling stepper motors? How to mate a stepper motor with LEGO Technic contraptions? How to efficiently control a RPi via ssh? How to efficiently control a RPi using a web framework? How to use vsketch to produce plot loops (automated or otherwise)? How to use doit to automate complex workflows? Etc.  This article is not a beginner tutorial either. A number of the topics covered here would, in tutorial form, warrant an even longer treatment by themselves. There should be enough information to understand how things work and understand the relevance of the tools I\u0026rsquo;ve used. Actually applying them in a project may, however, require some more focused reading. Likewise, the code is what I\u0026rsquo;d call \u0026ldquo;project quality\u0026rdquo;\u0026mdash;it does the job, I\u0026rsquo;m not too ashamed of it (mostly), but shouldn\u0026rsquo;t be construed as top-notch, state-of-the-art, production-ready copy-paste material.\nOverview Here is how the setup looks like in on my mobile plotting station:\nHere are the main components involved:\n An AxiDraw SE/A3 from Evil Mad Scientist Laboratories, driven by a Raspberry Pi 4 not visible in the picture (it\u0026rsquo;s neatly installed in the lower section of the plotting station). A Raspberry Pi 3 hooked to the High Quality Camera module and a EasyDriver stepper motor driver, held by a Manfrotto 241s Pump Cup and 244 Mini Friction Arm. An 80-mm paper feeder contraption made of LEGO Technic and a stepper motor. A Manfrotto ML840H On-Camera LED light, held by a couple of umbrella swivel adapters and a Phottix Multi Clamp.  Clearly, having a bunch of photography-related gear around is convenient for this kind of project! üòÑ\nHere is a schematics view of the same setup:\nThe most salient aspect is the use of two Raspberry Pis. This is entirely unnecessary\u0026mdash;a single RPi would be entirely sufficient. This arrangement happened to be more convenient for me because my plotting station already includes a RPi (with hostname axidraw.local) for my day-to-day use of the AxiDraw.\nMissing from both pictures is my computer, which I use to generate the frames and controls the plotting process by sending orders to both RPis via Wi-Fi.\nIn the next sections, I will dig into the details of many hardware and software aspects of this setup, culminating with the doit script which orchestrates everything ranging from generating the frame data, creating a simulated animation, controlling both RPis for plotting and picture acquisition, and assembling the final GIF:\nHardware Camera The camera assembly includes a Raspberry Pi (with hostname campi.local), the camera module and its optics, as well as the stepper motor driver. The three boards are assembled together using custom-cut plexiglass plates and nylon spacers. The camera module is connected to the RPi\u0026rsquo;s camera interface using the provided 200mm ribbon cable.\nI chose the High Quality Camera for the following reasons:\n It is made by the Raspberry Pi Foundation itself, so it has best-in-class software support out-of-the-box. With 12 megapixels and a large pixel size, it is one of the highest quality camera available for the Raspberry Pi. It uses interchangeable, C-mount lenses, which means that I can use a lens that\u0026rsquo;s best suited for this setup.  For the lens, I selected the 16mm telephoto. With its relatively narrow field of view, it minimises the distortions and can be placed high enough to leave space for the plotter to operate. With this lens, the positioning is mainly driven by the minimum focus distance, which is approximately 24cm, measured from the front-most part of the lens.\nHere is a sample frame and the corresponding raw image as taken by the camera (high-res version):\nNote that the image is rotated by 90¬∞ compared to the actual frame. It just happened to be easier to set up the camera this way. It will be automatically corrected before the final animation is assembled.\nStepper motor driver The campi.local RPi is also in charge of driving the paper feeder\u0026rsquo;s stepper motor via an EasyDriver board. As their name imply, stepper motors divide their full rotation into a number of discreet, equal steps. This makes them very convenient for use cases where accuracy and reproducibility is important, like 3D printers, plotters, and\u0026hellip; makeshift paper feeders. Stepper drivers generate the high power electrical signals needed to run the motor based on a simple GPIO inputs.\nHere is a close-up of the wiring schematic:\nThe main inputs are STEP and DIR. The motor turns by one step for each STEP pulse (transition from 0 to 1), while the rotation direction is controlled by DIR.\nThe ENABLE input controls whether the motor outputs are powered or not. When they are, the driver actively keeps the motor in its position, which tends to heat both the driver and motor itself. As the feeder spends most its time waiting for the plotter to draw the frame, it is good idea to drive ENABLE only when actually running the feeder.\nThe MS1 and MS2 inputs control the so-called \u0026ldquo;micro-stepping\u0026rdquo; capability of the driver. This feature increases the accuracy of the motor by further dividing the motor\u0026rsquo;s physical steps into up to 16 micro-steps. For a low-precision paper feeder like mine, this is not useful and the wiring can be skipped1.\nOne nice feature of the EasyDriver board is the ability to choose either 3.3V or 5V GPIO voltage. The default is 5V, but since the RPi uses 3.3V, I set the board to this voltage using a small solder bridge over the pads at the very bottom left of the board:\nLEGO structure My LEGO building skills aren\u0026rsquo;t anything to boast about\u0026mdash;a skilled builder would likely do a much better job. Yet, my contraption turned out to work rather reliably thanks to a few key design decisions.\nMost importantly, I wanted to leave the feeder \u0026ldquo;open\u0026rdquo; on the \u0026ldquo;top\u0026rdquo; side to minimise the chance of collision with the plotter or the pen during operations. The lower parallel structure is thus designed to maintain some rigidity between the paper roll part and the feed part. This actually worked much better that I anticipated!\nThese structures are really easy to build once your remember that 32 + 42 = 52:\nAll my diagonals use this arrangement and provide rigidity to the structure (purely rectangular structures are prone to parallelogram deformation).\nUsing the stepper motor weight to put pressure on the feeding wheel was another successful design. This ensured practically no paper slippage.\nFinally, remembering that the LEGO stud pitch is 8mm, a 10-stud distance is a good fit for 80mm paper rolls. These are easy to source in office supply shops.\nStepper-LEGO integration One might wonder why I used a custom stepper motor instead of native LEGO motors to drive the feeder. The decision boils down to two issues with LEGO motors:\n There are no stepper-type LEGO motors, so their accuracy is not great, nor is their power. Interfacing LEGO motors to a RPi is more complicated than using a stepper motor (especially if you already have stepper drivers lying around).  I actually tried to use a LEGO Mindstorm motor initially. I used a BrickPi board to interface the motor to the RPi. It worked, but as noted above these motors are not ideal for the task and ended up not being precise and reliable enough.\nAn alternative to the BrickPi board would be to use the actual Mindstorm controller. This is not ideal though, as the controller is battery powered and must be tinkered with to accept external power. Also, the RPi must connect to it via Bluetooth, and a specific Python package must be used.\nAll in all, in my experience, using a stepper motor is just easier.\nThis begs the question of how to physically interface the stepper motor with LEGO Technic parts.\nI used a 3D-printed adapter of my own design to interface the 5mm axle of my motor to Technic axles. There are many such designs available online for various shapes and sizes of motor axles.\nIt turns out that physically assembling the motor with LEGO Technic is easy enough when using the NEMA 17 form factor (one of the most commonly used by makers). The offset between their M3 attachments is 31mm. This is close enough to the 32mm offset between two 3-stud-apart Technic holes. Using conic screws easily takes up the remaining 1mm difference:\nCrucially, this method keeps the motor axle properly aligned with the Technic grid.\nSoftware Setting up the Raspberry Pi for friction-less SSH There is ample documentation available online on how to setup Raspberry Pis, which I won\u0026rsquo;t reproduce here. I\u0026rsquo;d like to focus instead on how to configure the RPi such as to minimise any friction when interacting with your RPi via SSH\u0026mdash;either manually or automatically.\nHere is what\u0026rsquo;s needed to achieve this:\n Unless you are using wired Ethernet, your home Wi-Fi must be configured on the RPi (of course, duh). SSH must be configured for key-based authentication. This removes the need to input a password when SSHing into your RPi, which is annoying with manual operations and precludes automation. ZeroConf/Bonjour/Avahi is installed and active. This means that you can connect to the RPi using a URL in the form of hostname.local instead of using an explicit IP address, which is hard to remember and subject to change. You computer must \u0026ldquo;know\u0026rdquo; which username to use when connecting to a given RPi.  For key-based authentication, you must first generate a public/private key pair on your computer (if you haven\u0026rsquo;t done so already). This is done using the following command:\nssh-keygen You\u0026rsquo;ll need to answer a few questions, which can all be left as default. This will create two files in your home directory:\n The private key: ~/.ssh/id_rsa The public key: ~/.ssh/id_rsa.pub  (Actual naming may differ depending on the type of key generated.)\nThe idea is to provide your public key (this is important\u0026mdash;not the private key!) to the RPi so that it can accept connections without requesting a password when the originator possesses the corresponding private key.\nMost of the configuration, fortunately, is addressed by the official Raspberry Pi Imager, which I strongly suggest using. Everything can be set in the \u0026ldquo;Advanced Options\u0026rdquo; dialog, including the hostname, the SSH public key, and the Wi-Fi credentials:\nAgain, note that the content of your public key (~/.ssh/id_rsa.pub) should be pasted in the relevant field.\nFinally, you need to tell your computer\u0026rsquo;s SSH that user pi (or whichever you chose) should be used by default when connecting to the given RPi. Create the ~/.ssh/config file if needed, and add the following content:\nHost axidraw.local User pi This tells your computer\u0026rsquo;s SSH to default to the username pi whenever it connects to axidraw.local.\nIf you already have a working RPi image, and you don\u0026rsquo;t want to recreate one from scratch, you can do the same configurations manually as follows:\n Check this article for Wi-Fi configuration. For key-based SSH authentication, create or edit the ~/.ssh/authorized_keys2 file on the RPi and add the content of your public key on a new line. Install ZeroConf/Bonjour/Avahi support with sudo apt-get install avahi-daemon. Change the hostname by running sudo raspi-config. The configuration is in the Network Options menu.  Here are the cool things that you can now do remotely without entering a password:\nscp file.svg axidraw.local: # copy a local file to your remote user directory scp file.svg axidraw.local:files/ # copy a local file the remote ~/files/ directory scp axidraw.local:files/file.svg ./ # copy a remote file locally ssh axidraw.local # log to your remote RPi ssh axidraw.local ls # list the files in your home directory cat my_file.svg | ssh axidraw.local axicli -L 2 -d 37 -u 60 -N -m plot # plot a file! This last one is particularly nice. SSH allows you to pipe the output of a local command (here cat just outputs the contents of my_file.svg) into a remote command\u0026rsquo;s input. This is a very powerful combination that I use for this project.\nControlling the camera and the feeder The campi.local Raspberry Pi has two missions: taking a picture of the frame after it is drawn, and run the motor to feed blank paper for the next frame. One of the easiest ways to make these functionalities remote controllable is to run a small HTTP server with two end-points, one for each task.\nThis is really easy to do with FastAPI (Flask would also work just as well). The corresponding code is available here, along with a requirements.txt file listing the required packages.\nTaking pictures For illustration, here is a shortened version with only the image acquisition part:\nfrom fastapi import FastAPI from fastapi.responses import FileResponse from picamera2 import Picamera2 # create a FastAPI server app = FastAPI() # setup and start the Pi camera for still frame acquisition picam2 = Picamera2() still_config = picam2.create_still_configuration(controls={\u0026#34;ExposureValue\u0026#34;: 0}) picam2.configure(still_config) picam2.start() # create a GET endpoint accepting a \u0026#34;ev\u0026#34; parameter and returning a JPG file @app.get(\u0026#34;/img/\u0026#34;) def get_picture(ev: int = 0): picam2.set_controls({\u0026#34;ExposureValue\u0026#34;: ev}) array = picam2.capture_file(\u0026#34;/tmp/temp.jpg\u0026#34;) return FileResponse(\u0026#34;/tmp/temp.jpg\u0026#34;, media_type=\u0026#34;image/jpeg\u0026#34;) All it takes to run the server is to the following command:\nuvicorn campi:app --host campi.local --port 8000 Uvicorn is a web server implementation fo Python uses FastAPI (in our case) to handle web requests. Here, campi:app tells Uvicorn to use the app object in the campi module (assuming our file is named campi.py).\nNote in passing how having ZeroConf setup with the RPi means that, once again, we don\u0026rsquo;t need to mess with explicit IP addresses. (Here, specifying a --host other than the default localhost is required to allow another computer to access the server.)\nWith the server running, any other computer on the local network can acquire a photo using the following command:\ncurl -s -o /tmp/test.png campi.local:8000/img/?ev=1 A couple of learnings from the field:\n Like in the example above, I\u0026rsquo;m using an exposure value of +1 with the #plotloop machine. This is because the white paper tend to lead to underexposure. With the code above, this parameter doesn\u0026rsquo;t seem to have effect until after the second picture is taken. So, after starting the server, I always use the command above to have one picture taken with ev=1 so the camera is primed when the machine actually uses it. The server can run for a long time, taking multiple hundreds of pictures (the rotating earth loop is 280-frame long). So it\u0026rsquo;s pretty important that the /img/ end-point doesn\u0026rsquo;t leak any memory. Especially uncompressed-frame-sized buffers. I actually ran into this issue with the first implementation, which used a memory buffer instead of a temporary file. After messing with tracemalloc for a while to figure this out, it ended up being just easier to switch implementation. For my first loop with the machine, the leak would fill my RPi\u0026rsquo;s memory and crash every 20 images or so\u0026mdash;it was painful process to go through the whole loop!  Moving the motor I use another end-point to control the feeder motor. This time, it includes a mandatory parameter in the URL: the approximate number of centimetre of paper to feed. Here is how it looks in the code:\n@app.get(\u0026#34;/motor/{cm}\u0026#34;) def run_motor(cm: int): ... Using this end-point is as easy as:\ncurl -s campi.local:8000/motor/5 The implementation is really boring. The gist of controlling a stepper motor is to toggle a GPIO up and down as many times as needed\u0026mdash;one per step. This could easily be done manually, but I\u0026rsquo;m using RpiMotorLib to reduce the code to a single line. The conversion from centimetre to step count depends on your motor (mine has 400 steps per full rotation, which is fairly standard) and the feeder wheel size. I have a magic number in the code that I tuned by trial-and-error.\nIt\u0026rsquo;s worth noting that the accuracy of the feeder mechanisms is not critical for this machine because the paper isn\u0026rsquo;t moved at all between the frame being drawn and the picture taken. The feeder has thus no impact on frame-to-frame alignment\u0026mdash;only the plotter\u0026rsquo;s repeatability matters here (which is a non-issue with the AxiDraw ‚ù§Ô∏è).\nAnother couple of things I learned on the way:\n As mentioned earlier, the motor and the driver can generate quite a bit of heat and power consumption when active, even when not moving. That\u0026rsquo;s why I manually toggle the ENABLE pin in the motor end-point. That way the motor is kept unpowered most of the time, and only activated when it must be moved. With the Raspberry Pi, it\u0026rsquo;s important to properly shut down the GPIO sub-system when exiting your program. It avoids some errors and minimise the risk for the hardware2. The proper way to do this with FastAPI is to implement a shutdown event handler: @app.on_event(\u0026#34;shutdown\u0026#34;) def shutdown_event(): GPIO.cleanup()   Using vsketch for plot loops Unsurprisingly, I\u0026rsquo;ve used vsketch to generate the animation frames. Before diving into the actual sketch I made for this article, I want to shortly digress on how to structure a sketch to produce SVGs suitable for plot loops.\nHere is a simple example of a plot loop sketch:\nimport math import vsketch class PlotloopSketch(vsketch.SketchClass): frame_count = vsketch.Param(50) frame = vsketch.Param(0) def draw(self, vsk: vsketch.Vsketch) -\u0026gt; None: vsk.size(\u0026#34;5x5cm\u0026#34;, center=False) vsk.scale(\u0026#34;cm\u0026#34;) radius = 2 vsk.circle(2.5, 2.5, radius=radius) angle = 360 / self.frame_count * self.frame vsk.circle( 2.5 + radius * math.cos(math.radians(angle)), 2.5 + radius * math.sin(math.radians(angle)), radius=0.1, ) def finalize(self, vsk: vsketch.Vsketch) -\u0026gt; None: vsk.vpype(\u0026#34;linemerge linesimplify reloop linesort\u0026#34;) This sketch (available in the vsketch repository) animates a small circle rotating around a larger circle3:\nThe first noteworthy aspect is its use of the two parameters: frame_count and frame. This makes it super easy to control the animation length and generate all the frames. For example, the frames for this GIF were generated with the following command:\nvsk save plotloop -m -p frame_count 13 -p frame 0..12 (Note the use of -m to use all available CPU cores.)\nAnother key element is to use center=False in the initial vsk.size() call. Without this, vsketch would auto-centre every frame based on the geometry and this would result in the animation wobbling around. I actually made a similar mistake with one of my first plot loop4:\nLast but not least, notice how the algorithm generates the exact same frame for frame = 0 and frame = frame_count5. This is obviously necessary to obtain a properly looping behaviour\u0026mdash;but is easier said than done for all but trivial examples.\nOne way is to use periodic trigonometric functions such as sine and cosine like I did in the example above. This is also the approach I used for the rotating Earth loop.\nMany generative artwork rely on Perlin noise. Some implementations may offer some kind of periodicity that could be used to generate a looping set of frames. Alternatively, periodicity can be achieved by sampling the noise field along cylindrical coordinates instead of on a cartesian grid.\nAn entire article could be written on this topic, and this one is long enough. Instead, I\u0026rsquo;ve added another example to the vsketch repository to illustrate this principle:\nThe rotating Earth sketch The full sketch code is too long to be reproduced in this article\u0026mdash;it\u0026rsquo;s available here in my sketches repository on GitHub. Instead, I will provide here an overview of how it\u0026rsquo;s implemented. You might want to open the code in another window to follow along.\nData preprocessing First, the data. I used the World Country boundaries from ESRI. It contains polygons for all countries in the world. By merging them with Shapely\u0026rsquo;s unary_union(), one can obtain the land/water boundaries. This happens in the build_world() function.\nHere is how the merged countries look after the union step:\nThere is one oddity I had to deal with, which explains the magic numbers and other ugliness in that function: Antarctica. This is the only body of land sitting over one of the poles, which are singularities in the latitude/longitude coordinate system.\nThe left is how the data looked like originally. On the right is the boundary once the artificial limit at ~80¬∞S is removed. In lat/long coordinates, it becomes a self-intersecting polygon. This can be dealt with a simple Shapely trick: create a LinearRing with the (self-intersecting) boundary and apply unary_union() on it. This creates a MultiLineString containing a corresponding list of non-intersecting linear rings.\nThis is the glitch that this procedure solved:\nAnother important step is to filter land masses by area, to avoid the myriads of tiny isles that would clutter the result and massively increase plotting time. You can\u0026rsquo;t just use the .area attribute from Shapely with lat/lon coordinates as this isn\u0026rsquo;t an equal-area projection, so I shamelessly copy/pasted polygon_area() from StackOverflow.\nThe final preprocessing step consist of converting the lat/lon land boundaries (stored in a Shapely Polygon instance) into 3D points on the unit sphere (stored as a Nx3 NumPy array). This is done by the project_polygon() function. The projected boundaries are stored in the LINES global variable.\nRendering the Earth The Earth rendering can be broken into the following steps:\n Rotate the Earth data as needed. Crop away the \u0026ldquo;far side\u0026rdquo; part of the data. Project orthogonally the rest of the data, a.k.a. drop the coordinate along which the backside was cropped and use the other two for drawing. Draw a circle :)  For rotation, I first compute 3 angles around the X, Y, and Z axes. These angles can be manually set for testing, or generated by trigonometric functions with different frequencies. Naturally, I make sure that these function are periodic with the frame count for a properly looping behaviour.\nI then generate 3 rotation matrices:\nrot_x = np.array( [ (1, 0, 0), (0, math.cos(math.radians(rot_x_angle)), -math.sin(math.radians(rot_x_angle))), (0, math.sin(math.radians(rot_x_angle)), math.cos(math.radians(rot_x_angle))), ] ) rot_y = np.array( [ (math.cos(math.radians(rot_y_angle)), 0, math.sin(math.radians(rot_y_angle))), (0, 1, 0), (-math.sin(math.radians(rot_y_angle)), 0, math.cos(math.radians(rot_y_angle))), ] ) rot_z = np.array( [ (math.cos(math.radians(rot_z_angle)), -math.sin(math.radians(rot_z_angle)), 0), (math.sin(math.radians(rot_z_angle)), math.cos(math.radians(rot_z_angle)), 0), (0, 0, 1), ] ) Finally, I combine them a single transformation matrix (did I mention I love NumPy?):\nrot = rot_x @ rot_y @ rot_z With that, rotating every single points of one of the land boundary line is just a matter of:\nrotated_line = (rot @ line.T).T Here, .T is used for transpose, and is needed for NumPy broadcasting rules to work. Remember that the actual calculation (3x3 matrix multiplication on every single points in line) happens in highly optimised C code, so this operation is fast.\nCropping is actually a bit trickier because you have to account for lines that may go from the front side to the far side and back, possibly multiple times. The cropping operation on a single line may thus result in multiple \u0026ldquo;sub-lines\u0026rdquo;.\nLuckily, I had already sorted out most this for vpype\u0026rsquo;s crop command. vpype\u0026rsquo;s API includes the crop_half_plane() function, which crops a line at a give location along one of the X or Y axis6. Adapting it to the 3rd dimension was trivial. While copy/pasting the function, I took along _interpolate_crop(), which deals specifically with computing the intersection of a line segment with the cropping plane.\nOnce the land boundary lines are cropped along one dimension, it\u0026rsquo;s a matter of drawing them using the other two dimensions using vsk.polygons. Which dimension is used for what is not very important since we\u0026rsquo;re dealing with a sphere. I made it so that when rotation angles are set to 0, the (0¬∞, 0¬∞) lat/lon point (somewhere in the Atlantic ocean, off Ghana) is dead in the center of the rendered Earth.\nIn the sketch code, you\u0026rsquo;ll also find controls to enable pixelation using vpype-pixelart. I ran some trials with it but decided against it\u0026mdash;to messy for this kind of line work.\nPutting it all together with doit At this point, all the #plotloop machine\u0026rsquo;s body parts are in place and just need a beating heart to set them in motion. doit is the perfect tool for this.\nAlthough doit is rather easy to use, it still has a tiny bit of a learning curve. If this is your first ever encounter with it, you might want to check the introductory article I recently wrote. This project takes this to a whole new level.\nInstead of looking at the dodo.py file line by line, I\u0026rsquo;ll first provide an overview of the workflows it implements (again, you might want to open the file in another window to follow along). Then, I\u0026rsquo;ll go deeper into a few, hand-picked topics to highlight interesting techniques.\nThe workflows The bulk of the dodo.py file implements two workflows using a bunch of tasks: one is to create a simulated animation based on the frames' SVG, and another to plot, photograph, post-process, and assemble the frames into the final animation.\nHere is a schematic of the workflows.\nLet\u0026rsquo;s review the tasks involved in creating the simulated animation:\n The generate task generates all the frame SVGs with a single call to vsketch. It is basically running some version of this command: vsk save . -m -p frame_count 280 -p frame 1..280 The outcome of this task is one SVG file per frame, numbered from 1 to 280. Each of the simframe:XXXX sub-task takes one frame SVG and convert it to a JPG with a _simulated prefix using librsvg\u0026rsquo;s rsvg-convert7. The sub-tasks are named after their corresponding frame number, e.g. simframe:0010 correspond to the frame number 10. Finally, the simulate task combines all the simulated frames into a single animated GIF, using ImageMagick\u0026rsquo;s convert command.  I\u0026rsquo;m calling this a workflow because each of these tasks have their file_dep and targets carefully defined. As a result, doit understands the dependency relationship between them. From a clean slate, calling doit simulate will first execute generate, then each of the simframe:XXXX sub-tasks, before finally running simulate to produce the animation.\nThe workflow for the actually plotted animation is similarly structured, but includes an additional post-processing step:\n  The workflow starts with the same generate task as before.\n  Each of the frame is then plotted and photographed by the corresponding plot:XXXX sub-task, which performs the following steps:\n Plot the frame by sending the SVG via SSH to axicli running on the axidraw.local RPi (as described earlier). Move the pen away by 3 inches to get it out of camera view (again, using SSH and axicli). Take a picture of the frame and download the corresponding image using curl (as described earlier). Move the pen back to its original position. Feed fresh paper using curl (as described earlier).  This is a good example of how a single doit task may execute multiple CLI commands.\n  Each frame is then post-processed by the corresponding postprocess:XXXX sub-task using convert. It rotates the image in the correct orientation, crops it tightly around the earth, converts it to grayscale, and bumps its brightness and contrast a bit. This is the command used:\nconvert frame_XXXX_plotted.jpg -rotate 270 -crop 1900x1900+605+785 \\  -colorspace Gray -brightness-contrast 5x15 frame_XXXX_postprocessed.jpg   Finally, the animation task combines all post-processed frames into the final animated GIF using convert.\n  One may wonder, why is the postprocess:XXXX task separate from the plot:XXXX task? The convert command could just as well be added to the list of commands plot:XXXX executes. The answer is to be able to tweak the post-processing step without invalidating the plotting process. If both tasks were merged, any modification to the post-processing (e.g. adjusting the cropping parameters) would require re-plotting the entire frame\u0026mdash;a lengthy process! This issue disappears with a stand-alone postprocess:XXXX task, which is very powerful when fine-tuning the workflow.\nBasic doit syntax Armed with this dodo.py file, we are now in complete control of our workflow.\nHere are a few example commands (I won\u0026rsquo;t list all the output here, check my other article for a gentler introduction on how it behaves).\nFirst, doit always provides a list of available tasks with doit list:\nanimation Make the animation. disable_xy Disable X/Y motors generate Generates golden master SVGs from the list of input files. plot Plot the plotter-ready SVGs. postprocess Post-process the plotted images. shutdown Shutdown everything simframe Simulate a frame. simulate Make the simulated animation. toggle Toggle pen up/down Generating the frame SVGs is a matter of:\ndoit generate This is not needed though, as this task is automatically run when executing other tasks depending on it. For example, this executes generate (if needed) and all the simframe:XXXX sub-tasks to produce the simulated GIF:\ndoit simulate A single simulated frame can be generated by specifying the frame number:\ndoit simframe:0118 This works because I chose to name sub-tasks after the corresponding (zero-padded) frame number.\nAll the frames can be generated at once by omitting the sub-task name:\ndoit simframe Likewise, producing the final, plotted animation is just a matter of running the following command and waiting 9 hours üï∞:\ndoit animation Executing a range of sub-tasks It is often useful to execute a range of sub-tasks. For example, early testing requires plotting, say, the first 10 frames to verify that everything works correctly. (Spoiler alert: it doesn\u0026rsquo;t! The process must be repeated several times until all the glitches are worked out.)\nThankfully, this is made very easy thanks to bash\u0026rsquo;s brace expansion syntax (it works the same with zsh and, probably, other shells8).\nHere is an illustration to demonstrate the idea:\necho {1..5} The braces with the .. syntax are interpreted by bash as a range that needs expansion. Accordingly, the output of the above is:\n1 2 3 4 5 The good news is that it understands zero-padding:\necho {0001..0015} This produces:\n0001 0002 0003 0004 0005 0006 0007 0008 0009 0010 0011 0012 0013 0014 0015 Knowing this, we can instruct doit to plot a specific frame range with the following command:\ndoit plot:{0005..0015} This expends to doit plot:0005 plot:0006 plot:0007 ..., which doit interprets as a list of tasks to be executed.\nThis syntax is beautifully consistent with both the single task form (doit plot:0012) and vsketch\u0026rsquo;s -p,--param option (vsk save -p frame 1..280). It\u0026rsquo;s also yet another shining example of how powerful terminals can be for automation.\nNote that, again, this is enabled by my choice of consistently naming sub-tasks after their zero-padded frame number.\nPath management This dodo.py file wrangles with a lot of different files. Each frame has up to 4 corresponding files (the original SVG, the simulated JPG, the plotted JPG, and the post-processed JPG), each with a specific suffix.\nA small helper class is useful to manage this complexity. Here is how it looks:\nimport pathlib PROJECT_DIR = pathlib.Path(__file__).parent FRAME_COUNT = 280 PIXELIZE = False PROJECT_NAME = \u0026#34;world\u0026#34; BASENAME = f\u0026#34;{PROJECT_NAME}_frame_count_{FRAME_COUNT}_pixelize_{PIXELIZE}\u0026#34; class FileSpec: def __init__(self, frame: int): self.frame = frame directory = PROJECT_DIR / \u0026#34;output\u0026#34; # vsketch doesn\u0026#39;t add zero padding to frame number self.source = directory / (BASENAME + f\u0026#34;_frame_{self.frame}.svg\u0026#34;) # for the other file we add the zero padding to keep the order with CLI tools base_frame = BASENAME + f\u0026#34;_frame_{self.frame:04d}\u0026#34; self.simulated = directory / (base_frame + \u0026#34;_simulated.jpg\u0026#34;) self.plotted = directory / (base_frame + \u0026#34;_plotted.jpg\u0026#34;) self.postprocessed = directory / (base_frame + \u0026#34;_postprocessed.jpg\u0026#34;) FILE_SPECS = {i: FileSpec(i) for i in range(1, FRAME_COUNT + 1)} FileSpec instances are created based on a frame number, and contain all the paths related to the corresponding frame. This is easy to implement using the pathlib module from the standard library.\nNote that vsketch doesn\u0026rsquo;t zero-pad the frame number in the SVGs it produces. This is a bit unfortunate as it messes up alphabetical ordering for most CLI tools (for example, this creates frame ordering issues when creating a GIF from a bunch of JPGs). All the other paths are constructed with zero-padding.\nAlso notice the PIXELIZE global variable. Its value is forwarded as a parameter to vsketch, which controls whether vpype-pixelize should be used. As noted above, I\u0026rsquo;ve decided against it for this project. In any case, I made sure to reflect this value in all the files' base name to avoid confusion by doit when changing the parameter\u0026rsquo;s value.\nThe FILE_SPECS global variable contains a dictionary which maps the FileSpec instance to the corresponding frame number. This simplifies a lot the task implementation.\nTask dependencies As I explained in my previous article, proper handling of task targets and dependencies is key for doit to understand the structure of the workflows and be smart about which task must be executed when.\nLet\u0026rsquo;s consider the simframe:XXXX sub-tasks as an example:\ndef task_simframe(): \u0026#34;\u0026#34;\u0026#34;Simulate a frame.\u0026#34;\u0026#34;\u0026#34; for frame, spec in FILE_SPECS.items(): yield { \u0026#34;name\u0026#34;: f\u0026#34;{frame:04d}\u0026#34;, \u0026#34;actions\u0026#34;: [ f\u0026#34;rsvg-convert -b white -h 200 {spec.source}\u0026gt; {spec.simulated}\u0026#34;, ], \u0026#34;file_dep\u0026#34;: [spec.source], \u0026#34;targets\u0026#34;: [spec.simulated], \u0026#34;clean\u0026#34;: True, } Each sub-task has the source SVG file set as file_dep, and the simulated JPG file as targets. Since this stage of the workflow is structured as sub-tasks\u0026mdash;one sub-task per frame\u0026mdash;each sub-task focuses on a single input and output file.\nIn contrast, the simulate task needs all the simulated frames to create a single animated GIF:\ndef task_simulate(): \u0026#34;\u0026#34;\u0026#34;Make the simulated animation.\u0026#34;\u0026#34;\u0026#34; file_list = [spec.simulated for spec in FILE_SPECS.values()] paths = \u0026#34; \u0026#34;.join(str(file) for file in file_list) target = PROJECT_DIR / \u0026#34;output\u0026#34; / f\u0026#34;{BASENAME}.gif\u0026#34; return { \u0026#34;actions\u0026#34;: [f\u0026#34;convert -delay 5 -loop 0 {paths}{target}\u0026#34;], \u0026#34;file_dep\u0026#34;: file_list, \u0026#34;targets\u0026#34;: [target], \u0026#34;clean\u0026#34;: True, } In this case, file_dep contains all the simulated frames, while targets has the animated GIF.\nThese snippets also illustrate how the FileSpec class and the FILE_SPECS global variable simplify the task implementation.\nDepending on dodo.py Adding the dodo.py file to the dependency list might sound like a smart idea. Indeed, a modification of the dodo.py may potentially invalidate all generated files.\nIn our case, this can be done in the generate task, on which all other tasks depend:\ndef task_generate(): \u0026#34;\u0026#34;\u0026#34;Generates golden master SVGs from the list of input files.\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;actions\u0026#34;: [ ( f\u0026#34;vsk save -n {PROJECT_NAME}-p frame_count {FRAME_COUNT}\u0026#34; f\u0026#34;-p pixelize {PIXELIZE}-p frame 1..{FRAME_COUNT}-m .\u0026#34; ) ], \u0026#34;file_dep\u0026#34;: [PROJECT_DIR / f\u0026#34;sketch_{PROJECT_NAME}.py\u0026#34;, __file__], # \u0026lt;-- LOOK HERE \u0026#34;targets\u0026#34;: list(spec.source for spec in FILE_SPECS.values()), \u0026#34;clean\u0026#34;: True, } The path of the dodo.py file is conveniently stored in the __file__ global variable by the Python interpreter, so it is a matter of adding it to the file_dep.\n(Notice, in passing, that the sketch file is also listed as file_dep. This triggers a complete rebuild whenever the sketch file is modified.)\nThis technique is useful in the beginning to ensure that intermediate files potentially contaminated by bugs of an early, in-construction dodo.py file are properly rebuilt when the bug is fixed. It can, however, become an annoyance later on. Executing all of the plot:XXXX sub-tasks takes about 9 hours in total. You really don\u0026rsquo;t want to repeat the whole thing just because of a code formatting fix or added comment in dodo.py! As such, the dodo.py is not listed as dependency in the file on GitHub.\nHelper tasks In addition to the main workflows described so far, the dodo.py file includes a few helper tasks to toggle the pen up/down, disable the motor power (which is always better when the keeping the AxiDraw powered but unused for long periods of time), and shutdown everything (the RPis and the AxiDraw).\nThe code is really straightforward, but very handy to have around. It\u0026rsquo;s basically a thin wrapper over axicli functionalities:\ndef task_toggle(): \u0026#34;\u0026#34;\u0026#34;Toggle pen up/down\u0026#34;\u0026#34;\u0026#34; return {\u0026#34;actions\u0026#34;: [f\u0026#34;{AXICLI}-m toggle\u0026#34;]} def task_disable_xy(): \u0026#34;\u0026#34;\u0026#34;Disable X/Y motors\u0026#34;\u0026#34;\u0026#34; return {\u0026#34;actions\u0026#34;: [f\u0026#34;{AXICLI}-m manual -M disable_xy\u0026#34;]} def task_shutdown(): \u0026#34;\u0026#34;\u0026#34;Shutdown everything\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;actions\u0026#34;: [ \u0026#34;ssh campi.local sudo poweroff\u0026#34;, \u0026#34;ssh axidraw.local sudo poweroff\u0026#34;, ], \u0026#34;task_dep\u0026#34;: [\u0026#34;disable_xy\u0026#34;], } Notice the use of task_dep in the shutdown task. It basically says that disable_xy should be executed whenever shutdown is called.\nFinal words And that wraps up what I wanted to cover about this project. This was waaay longer than I anticipated! I hope you enjoyed it and, possibly, learned something.\nI must once again credit Simon Willison for the inspiration and his exhortation to blog about projects. Here, I took the advice, and pushed it to the extreme! üòÖ I don\u0026rsquo;t expect to be as thorough every time, but will certainly continue to produce some coverage my future endeavours.\nPlease hit me up in the comments or on social medias with any feedback you may have\u0026mdash;or just to let me know you made it this far üèÜ, it\u0026rsquo;ll make my day! ‚ù§Ô∏è\n  I did wire MS1 and MS2 in my setup to run a few tests, since RpiMotorLib supports this feature.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n An I/O in output mode is at risk of having its driver damaged when short-circuited. For this reason, all I/Os are in input mode by default.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The fact that it resemble an orbiting planet is actually fortuitous!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n In this case, this was caused by using vpype\u0026rsquo;s layout command on each frame. The effect is the same as center=True though.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Or, more generally, for frame = k and frame = k + n * frame_count.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The crop command implementation basically consists of calling crop_half_plane() four times to cut away geometries outside the target rectangular area.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I found that rsvg-convert is more robust with SVG than ImageMagick.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n As a reminder, a \u0026ldquo;shell\u0026rdquo; (like bash, zsh, tcsh, etc.) is the program that prints the terminal prompt and interprets the commands you type, launching processes as required. If a terminal window is like a small computer\u0026rsquo;s screen, then the shell is that computer\u0026rsquo;s operating system.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://bylr.info/articles/2022/12/22/automatic-plotloop-machine/","summary":"Deep dive into the internals of my Automatic","title":"Project: the Automatic #plotloop Machine"},{"content":"doit (a.k.a. PyDoIt) is a fantastic Python-based tool to automate repetitive workflows. It works particularly well alongside vpype to address mundane plotting-related tasks. This article explains in details how to automate an SVG optimisation and conversion workflow.\nMost plotter workflows involve one or more repetitive steps which, when executed manually, take time, are boring, and possibly error-prone. Here are some examples that come to mind:\n Optimizing SVGs using vpype\u0026rsquo;s linemerge reloop linesort linesimplify commands. Converting SVGs into a format your plotter understands (e.g. HPGL, or G-code using vpype-gcode). Splitting multi-layer SVGs into individual layers (e.g. if this is a requirement of your plotter for multi-colour plots). Making a PNG version of SVGs for archival purposes. Running the axicli command to plot an SVG with an Axidraw. Uploading optimised files to the computer/server/Raspberry Pi in control of your plotter. Etc.  Not only your workflow may include one or more of these steps, but you may need to apply it on a single SVG at a time, or on a bunch of them at once. Even better, you might want to apply your workflow only on SVGs which were updated or created since the last execution.\nYou can do exactly that with doit\u0026mdash;let\u0026rsquo;s see how.\nInstalling doit Although its documentation sadly doesn\u0026rsquo;t mention it, pipx is the best way to install doit (as for vpype):\n$ pipx install doit You can check that the installation was successful by running this command:\n$ doit --version 0.36.0 lib @ /Users/\u0026lt;username\u0026gt;/.local/pipx/venvs/doit/lib/python3.10/site-packages/doit Basics As a starting point, let\u0026rsquo;s assume you have a bunch of SVGs which need optimising before plotting, stored in a originals subdirectory. Save the optimisation commands in a VPY file named optimize.vpy, with the following content:\nlinemerge reloop linesort linesimplify Then, create a subdirectory named processed, which will contain the optimised SVGs:\n$ mkdir processed Here is how your file hierarchy should look like:\n. ‚îú‚îÄ‚îÄ optimize.vpy ‚îú‚îÄ‚îÄ originals/ ‚îÇ ‚îú‚îÄ‚îÄ dots.svg ‚îÇ ‚îú‚îÄ‚îÄ halftone.svg ‚îÇ ‚îî‚îÄ‚îÄ hline.svg ‚îî‚îÄ‚îÄ processed/ Our goal is to have doit automate the optimisation of the source SVGs in originals, and store the result in processed.\ndoit operates by loading a description of the task(s) it must execute, typically in a file named dodo.py1. As the name suggests, the content of this file is Python code.\nCreate a dodo.py file with the following content:\nimport pathlib # (1) DIR = pathlib.Path(__file__).parent # (2) SOURCES = list((DIR / \u0026#34;originals\u0026#34;).glob(\u0026#34;*.svg\u0026#34;)) # (3) VPY = DIR / \u0026#34;optimize.vpy\u0026#34; # (4) def task_optimize(): # (5) \u0026#34;\u0026#34;\u0026#34;optimize SVGs\u0026#34;\u0026#34;\u0026#34; # (6) for source in SOURCES: # (7) optimized = DIR / \u0026#34;processed\u0026#34; / (source.stem + \u0026#34;_optimized.svg\u0026#34;) # (8) yield { # (9) \u0026#34;name\u0026#34;: source.stem, # (10) \u0026#34;actions\u0026#34;: [ f\u0026#34;vpype read \u0026#39;{source}\u0026#39; -I \u0026#39;{VPY}\u0026#39; write \u0026#39;{optimized}\u0026#39;\u0026#34; # (11) ], } Let\u0026rsquo;s examine this code line-by-line.\n The pathlib built-in module is great at file wrangling. Check this Real Python article for a gentle yet thorough introduction. Here we use it to find our project directory, which is the parent of the present file, whose path is stored in the __file__ variable by the Python interpreter. We list all the SVGs contained in the originals subdirectory, and store them in the SOURCES variable. Note that glob() returns a generator, which must be converted to a list if SOURCES is to be iterated multiple times. We keep the path to the optimize.vpy file in the VPY variable. Python functions with name starting with task_ are interpreted by doit as tasks. Here we have just one. Let\u0026rsquo;s call it \u0026ldquo;optimize\u0026rdquo;, thus the task_optimize() function name. The function\u0026rsquo;s docstring is used by doit as help string for the task, so it is useful to include one. Task functions must return one or more Python dictionaries describing the task. In our case, we want to create one sub-tasks per source SVG file. For each source SVG, we derive the path for the corresponding optimised SVG. The optimised SVG are located in the processed subdirectory and have a _optimized.svg suffix to their name. Using yield keyword (instead of return) makes our function a generator (gentle introduction available here). This is a convenient way to return (er\u0026hellip; yield) multiple objects, which is supported by doit. Here, we yield one dictionary per sub-task. Sub-tasks must be individually named so that they can be distinguished. Here we derive the sub-task name from the source SVG filename. For example, the sub-task corresponding to my_file.svg will be named my_file, and can be referred to with doit as optimize:my_file. Last but not least, the \u0026quot;actions\u0026quot; entry of the sub-task dictionary lists the actions to be performed by the task. doit interprets strings as shell commands, so we build a vpype pipeline to optimise the source SVG using our VPY and saving the result in the desired location. For example, for my_file.svg, the action will be vpype read originals/my_file.svg -I optimize.vpy write processed/my_file_optimized.svg2.  Let\u0026rsquo;s take a step back to properly understand what\u0026rsquo;s going on.\nThe function task_optimize() produces a task description\u0026mdash;it does not actually run the task. When we run doit (using the doit command), it loads the dodo.py file, notices that it contains a task function, and calls it to learn about that task. It\u0026rsquo;s only then that it can decide which action(s) to actually execute, based on the task description. In this case, the actions are the vpype pipelines stored in the \u0026quot;actions\u0026quot; entries.\nAlthough this dodo.py file is not overly complicated, it can still feel like quite some work compared to, you know, just calling vpype manually. I certainly felt so when first using doit. So let\u0026rsquo;s see what we gained by going through this effort.\nFirst and foremost, we now have a potent batch processing system. We can optimise all of our source SVGs by telling doit to execute the optimize task:\n$ doit optimize . optimize:dots . optimize:halftone . optimize:hline Here is the result after running this command:\n. ‚îú‚îÄ‚îÄ dodo.py ‚îú‚îÄ‚îÄ optimize.vpy ‚îú‚îÄ‚îÄ originals/ ‚îÇ ‚îú‚îÄ‚îÄ dots.svg ‚îÇ ‚îú‚îÄ‚îÄ halftone.svg ‚îÇ ‚îî‚îÄ‚îÄ hline.svg ‚îî‚îÄ‚îÄ processed/ ‚îú‚îÄ‚îÄ dots_optimized.svg ‚îú‚îÄ‚îÄ halftone_optimized.svg ‚îî‚îÄ‚îÄ hline_optimized.svg doit indeed created properly-named, optimised versions of the source SVGs in the processed directory! üéâ\nSince we only have just one task defined, we don\u0026rsquo;t even need to specify its name:\n$ doit . optimize:dots . optimize:halftone . optimize:hline You can also specify a specific sub-task to execute:\n$ doit optimize:halftone . optimize:halftone Pretty neat already\u0026mdash;but there is a lot more to gain with a little more effort!\nHandling targets and dependencies Playing with the commands above, you may notice that each call of the optimize task triggers the processing of the corresponding SVGs\u0026mdash;even if said SVGs were already processed before. The reason for this is that doit doesn\u0026rsquo;t yet know what the task inputs and outputs are, so it cannot check whether that output exists or is outdated. So, to be on the safe side, it always executes all specified tasks every time.\nBy letting doit know about tasks' inputs and outputs, doit can be much smarter about what it actually needs to do.\nIn doit parlance, the file(s) a task uses as input are called dependencies (\u0026quot;file_dep\u0026quot; entry). Likewise, the file(s) created as output are called targets (\u0026quot;targets\u0026quot; entry). By specifying what these are in the dodo.py file, doit can decide whether the target of a given task needs to be generated or not, saving a lot of time when repeating the workflow.\nUpdate the dodo.py file as follows:\nimport pathlib DIR = pathlib.Path(__file__).parent SOURCES = list((DIR / \u0026#34;originals\u0026#34;).glob(\u0026#34;*.svg\u0026#34;)) VPY = DIR / \u0026#34;optimize.vpy\u0026#34; def task_optimize(): \u0026#34;\u0026#34;\u0026#34;optimize SVGs\u0026#34;\u0026#34;\u0026#34; for source in SOURCES: optimized = DIR / \u0026#34;processed\u0026#34; / (source.stem + \u0026#34;_optimized.svg\u0026#34;) yield { \u0026#34;name\u0026#34;: source.stem, \u0026#34;actions\u0026#34;: [ f\u0026#34;vpype read \u0026#39;{source}\u0026#39; -I \u0026#39;{VPY}\u0026#39; write \u0026#39;{optimized}\u0026#39;\u0026#34; ], \u0026#34;targets\u0026#34;: [optimized], # (1) \u0026#34;file_dep\u0026#34;: [source, VPY], # (2) }  The \u0026quot;targets\u0026quot; entry is a list of all the files generated by the sub-task. In our case, there is only one, whose path is stored in the optimized variable. The \u0026quot;file_dep\u0026quot; entry is a list of all the files the sub-task depends on. In our case, both the source SVG and the VPY file are involved to create an optimised SVG, so we list them both.  It would be easy to forget the VPY file in the \u0026quot;file_dep\u0026quot; entry. That would be a mistake. All the optimised SVGs should be regenerated when the VPY file is modified. For doit to realise this, we must list the VPY file as a dependency.\nWith the modification above, doit now knows when to run optimisation sub-tasks and when they can be skipped.\nLet\u0026rsquo;s experiment with a clean slate by deleting all the processed files:\n$ rm processed/*.svg doit must now execute all sub-tasks:\n$ doit . optimize:dots . optimize:halftone . optimize:hline Notice the dot (.) prefixing each line and how the execution is relatively slow.\nNow, this is what happens if we run doit again:\n$ doit -- optimize:dots -- optimize:halftone -- optimize:hline Execution time is now much faster and each line is now prefixed with --, indicating that doit skipped the corresponding sub-task.\nLet\u0026rsquo;s see what happens if one of the source file is modified.\n$ echo \u0026#34; \u0026#34; \u0026gt;\u0026gt; originals/halftone.svg $ doit -- optimize:dots . optimize:halftone -- optimize:hline We first append a single space to the halftone.svg (which is harmless on a valid SVG) to simulate a change3. As expected, doit rebuilds the of halftone.svg without running the other tasks! üéâ\nWe now have a setup able to automatically process large batches of files and be smart about if/when any sub-task must be repeated. You have a thousand SVGs to process? It\u0026rsquo;s coffee time while the CPUs churn through them4. You add just one to the list? Instant results, thanks to doit!\nCleaning up The files created by the optimize task can be considered \u0026ldquo;temporary\u0026rdquo;. When missing, they are automatically recreated by doit, and are overwritten by a new version when the input file (or the VPY file) change. In that sense, they matter much less than the source SVGs and the dodo.py file, which collectively form the \u0026ldquo;recipe\u0026rdquo; to build the optimised SVGs5.\nThe ability to delete these files may occasionally be useful. For example, to force a complete rebuild of the optimised files, to make an archive with only the true source files, or simply to free some disk space.\ndoit provides this feature with a single modification to the dodo.py file:\nimport pathlib DIR = pathlib.Path(__file__).parent SOURCES = list((DIR / \u0026#34;originals\u0026#34;).glob(\u0026#34;*.svg\u0026#34;)) VPY = DIR / \u0026#34;optimize.vpy\u0026#34; def task_optimize(): \u0026#34;\u0026#34;\u0026#34;optimize SVGs\u0026#34;\u0026#34;\u0026#34; for source in SOURCES: optimized = DIR / \u0026#34;processed\u0026#34; / (source.stem + \u0026#34;_optimized.svg\u0026#34;) yield { \u0026#34;name\u0026#34;: source.stem, \u0026#34;actions\u0026#34;: [ f\u0026#34;vpype read \u0026#39;{source}\u0026#39; -I \u0026#39;{VPY}\u0026#39; write \u0026#39;{optimized}\u0026#39;\u0026#34; ], \u0026#34;targets\u0026#34;: [optimized], \u0026#34;file_dep\u0026#34;: [source, VPY], \u0026#34;clean\u0026#34;: True, # (1) }  Tell doit that target files should be deleted when running doit clean.  Let\u0026rsquo;s see this in action:\n$ doit clean optimize:hline - removing file \u0026#39;.../processed/hline_optimized.svg\u0026#39; optimize:halftone - removing file \u0026#39;.../processed/halftone_optimized.svg\u0026#39; optimize:dots - removing file \u0026#39;.../processed/dots_optimized.svg\u0026#39; Works as expected! üéâ\nMultiple tasks Although doit already shines dealing with a single task, it reveals its true power when multiple tasks are involved\u0026mdash;even more so when they depend on each other.\nFor the illustration purposes, let\u0026rsquo;s imagine that we need to convert the optimised SVGs to HPGL, so that we may plot them on a shiny \u0026lsquo;83 HP 7475a. We\u0026rsquo;ll add a second task for this6.\nFirst, let\u0026rsquo;s start by creating a new hpgl subdirectory to store the HPGL files:\n$ mkdir hpgl Since we cleaned the optimised SVGs in the previous steps, this how your project directory should look:\n. ‚îú‚îÄ‚îÄ dodo.py ‚îú‚îÄ‚îÄ hpgl/ ‚îú‚îÄ‚îÄ optimize.vpy ‚îú‚îÄ‚îÄ originals/ ‚îÇ ‚îú‚îÄ‚îÄ dots.svg ‚îÇ ‚îú‚îÄ‚îÄ halftone.svg ‚îÇ ‚îî‚îÄ‚îÄ hline.svg ‚îî‚îÄ‚îÄ processed/ Now, update the dodo.py file with the following content:\nimport pathlib DIR = pathlib.Path(__file__).parent SOURCES = list((DIR / \u0026#34;originals\u0026#34;).glob(\u0026#34;*.svg\u0026#34;)) VPY = DIR / \u0026#34;optimize.vpy\u0026#34; def optimized_path(source: pathlib.Path): # (1) \u0026#34;\u0026#34;\u0026#34;derive optimized path from source path\u0026#34;\u0026#34;\u0026#34; return DIR / \u0026#34;processed\u0026#34; / (source.stem + \u0026#34;_optimized.svg\u0026#34;) def hpgl_path(source: pathlib.Path): # (2) \u0026#34;\u0026#34;\u0026#34;derive HPGL path from source path\u0026#34;\u0026#34;\u0026#34; return DIR / \u0026#34;hpgl\u0026#34; / (source.stem + \u0026#34;.hpgl\u0026#34;) def task_optimize(): \u0026#34;\u0026#34;\u0026#34;optimize SVGs\u0026#34;\u0026#34;\u0026#34; for source in SOURCES: optimized = optimized_path(source) # (3) yield { \u0026#34;name\u0026#34;: source.stem, \u0026#34;actions\u0026#34;: [ f\u0026#34;vpype read \u0026#39;{source}\u0026#39; -I \u0026#39;{VPY}\u0026#39; write \u0026#39;{optimized}\u0026#39;\u0026#34; ], \u0026#34;file_dep\u0026#34;: [source, VPY], \u0026#34;targets\u0026#34;: [optimized], \u0026#34;clean\u0026#34;: True, } def task_hpgl(): \u0026#34;\u0026#34;\u0026#34;convert to HPGL\u0026#34;\u0026#34;\u0026#34; for source in SOURCES: # (4) optimized = optimized_path(source) # (5) hpgl = hpgl_path(source) yield { \u0026#34;name\u0026#34;: source.stem, \u0026#34;actions\u0026#34;: [ f\u0026#34;vpype read \u0026#39;{optimized}\u0026#39; write -d hp7475a -p a4 -q -c \u0026#39;{hpgl}\u0026#39;\u0026#34; ], \u0026#34;file_dep\u0026#34;: [optimized], # (6) \u0026#34;targets\u0026#34;: [hpgl], # (7) \u0026#34;clean\u0026#34;: True, } Let\u0026rsquo;s examine the changes one-by-one.\n To clean things up and avoid code duplication, we factored in optimized_path() the code to derive the path of an optimised SVG from a source SVG. We do the same to derive the path of an HPGL output from a source SVG in the hpgl_path() function. Note that neither of these function names start with task_, so they aren\u0026rsquo;t interpreted as tasks by doit. The only change to the optimize task is to use the optimized_path() helper function. This part is interesting. The purpose of the hpgl task is to convert optimised SVG into HPGL files, yet we iterate over the source SVGs instead. The reason is, for our purposes, SOURCES is our master \u0026ldquo;TODO list\u0026rdquo;. Everything the hpgl task must do is indirectly due to the presence of source SVGs. The source path is used only to derive the paths for the optimised SVG as well as the HPGL output. In particular, notice how source is not used anywhere in the return dictionaries. The optimised SVGs is now a dependency (as opposed to a target in the optimize task). Instead, the target is the HPGL file.  These two tasks collectively form a \u0026ldquo;pipeline\u0026rdquo;. The output (or target) of the first task corresponds to the input (or dependency) of the second. doit understands that thanks to the \u0026quot;file_dep\u0026quot; and \u0026quot;targets\u0026quot; entries being properly populated\u0026mdash;and can now be smart about it!\nLet\u0026rsquo;s take it for a spin by executing the hpgl task:\n$ doit hpgl . optimize:dots . optimize:halftone . optimize:hline . hpgl:dots . hpgl:halftone . hpgl:hline doit knows that it needs optimised SVGs to create HPGL file, so it automatically executes the optimize task.\nLet\u0026rsquo;s remove a single HPGL file to test what happens. This can be done using the doit clean command:\n$ doit clean hpgl:hline hpgl:hline - removing file \u0026#39;.../hpgl/hline.hpgl\u0026#39; This is what happens when we run the hpgl task again:\n$ doit hpgl -- optimize:dots -- optimize:halftone -- optimize:hline -- hpgl:dots -- hpgl:halftone . hpgl:hline The optimised version of hline.svg is still present and up-to-date, so the corresponding task is skipped. Only the HPGL conversion is executed.\nNow, let\u0026rsquo;s change one of the source files, like we did earlier:\n$ echo \u0026#34; \u0026#34; \u0026gt;\u0026gt; originals/dots.svg $ doit hpgl . optimize:dots -- optimize:halftone -- optimize:hline . hpgl:dots -- hpgl:halftone -- hpgl:hline doit correctly runs both the optimize and hpgl sub-tasks for the corresponding file! üéâ\nHelper tasks Tasks don\u0026rsquo;t have to be part of an intricate pipeline with carefully specified targets and dependencies. They can also be just a nice little helper that encapsulate a useful shell command.\nConsider for example this task, which can readily be added to our dodo.py file:\ndef task_show(): \u0026#34;\u0026#34;\u0026#34;display SVG\u0026#34;\u0026#34;\u0026#34; for source in SOURCES: yield { \u0026#34;name\u0026#34;: source.stem, \u0026#34;actions\u0026#34;: [f\u0026#34;vpype read {source}show\u0026#34;], } Its action consist of loading the source SVG and displaying it with vpype. This isn\u0026rsquo;t necessarily part of your workflow, but is convenient to have handy:\n$ doit show:dots The corresponding SVG is displayed by the vpype viewer:\nThis example is taken from vpype-perspective, where all the README\u0026rsquo;s figures are made from VPYs files stored in the repository\u0026rsquo;s examples/figures subdirectory. The conversion of these VPYs into SVGs is handled by doit using this dodo.py file. It\u0026rsquo;s a nice example of what can be done with doit.\nFinal words If you made it that far, I hope you are convinced of how useful doit is for workflow automation.\nIn this article, I focused on vpype, but doit can be used for entirely different things. As a matter of fact, I used it to automate my #plotloop machine, which I\u0026rsquo;ve described in my next article.\n   One of doit drawbacks is the fact that its dodo.py file is written in Python. Creating one requires at least some Python basics\u0026mdash;or willingness to acquire them. This might put off people uninterested by code.\nBut this is also its greatest strength. You wield the full power of Python when writing your dodo.py file, without any of the constraints of configuration languages such as YAML or TOML. This extends the possibilities much further than what was covered here, and makes learning doit a great investment! üéØ\nReady to take the plunge? I\u0026rsquo;m happy to help\u0026mdash;just share details of your workflow in the comments üëá, on Twitter/Mastodon, or on the Drawingbots Discord.\nEdit: TIL what \u0026ldquo;dolt\u0026rdquo; (lowercase L) means üòÖ, and changed DoIt (uppercase i) into doit, consistently with their documentation.\nEdit: Added a link to the Automatic #plotloop Machine article and updated the video.\n  The file may also have a different name, or be located elsewhere, but then its path should be provided to doit. Using dodo.py is simpler because this file is automatically detected and loaded by doit.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The code actually generates full paths.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n If you are used to make and similar systems, you might be tempted to touch originals/halftone.svg to trigger a rebuild instead of modifying the file\u0026rsquo;s content. This doesn\u0026rsquo;t work with doit as it uses a local database and file hashes instead of modification date to track dependencies.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n By the way, you can parallelise the processing of large batches using doit -n 8 optimize, where 8 is the number of CPU cores to use.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This bears strong similarities with software build systems, where compiled object files are created from source code by the compiler. As a matter of fact, doit can serve as a build system.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This example is slightly over-engineered. vpype can optimise and export to HPGL in one command, so technically a single doit task is needed. Even if multiple commands were required (vpype or otherwise), they can all be listed in a single doit task\u0026mdash;the \u0026quot;actions\u0026quot; entry is a list which can contain multiple items. It is still a relevant illustration for the many instances were multiple doit tasks are indeed useful.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  ","permalink":"https://bylr.info/articles/2022/11/10/batch-processing-doit-vpype/","summary":"\u003cp\u003e\u003ca href=\"https://pydoit.org\"\u003e\u003cem\u003edoit\u003c/em\u003e\u003c/a\u003e (a.k.a. PyDoIt) is a fantastic Python-based tool to automate repetitive workflows. It works particularly well alongside \u003ca href=\"https://vpype.readthedocs.io\"\u003e\u003cem\u003evpype\u003c/em\u003e\u003c/a\u003e to address mundane plotting-related tasks. This article explains in details how to automate an SVG optimisation and conversion workflow.\u003c/p\u003e","title":"Batch processing SVGs with DoIt and vpype"},{"content":"Excuse me\u0026hellip; Running a what on what terminal what? ü§î\nSo, here is the thing. A Raspberry PI paired with a touch-screen can serve as a touch-based, human-machine interface (HMI) for things like DIY projects, robots, and whatnot. For example, this is how I control my Axidraw plotter. The current implementation of my HMI software ‚Äì named taxi ‚Äì uses Kivy. Here it is in action:\n   This requires a full X11 and desktop environment (e.g. LXDE, used by default by Raspberry Pi OS), which is heavy (the full Raspberry Pi OS doesn\u0026rsquo;t fit on a 4GB memory card) and harder to turn into kiosk mode.\nEver since I first learned of the modern TUI framework Textual, I\u0026rsquo;ve been thinking of using it as a lightweight, kiosk-mode, touch-based HMI with minimal requirements for the underlying OS. All it needs is a terminal!\nMy first idea was to simply use the Linux console, which is the terminal-y thing you see at boot, and can use to login if you don\u0026rsquo;t have a X11/desktop environment. Unfortunately, due to severe limitations in the number of glyphs it can handle (or of my comprehension of it), the results were\u0026hellip; underwhelming:\nThis screenshot was made with a Debian VM running in Parallels Desktop, but the it\u0026rsquo;s the same for a Raspberry Pi and a physical screen.\nThen I learned about framebuffer terminal emulators. The Linux framebuffer is a subsystem to display on-screen graphics over the system console without relying on a X11 server. Framebuffer terminal emulators basically emulate a terminal and \u0026ldquo;draw\u0026rdquo; it to the screen using the Linux framebuffer. As it turns out, they are extremely niche pieces of software, so it felt like a trip to the past to get them to run! üòÖ\nFor a bunch of reasons discussed below, this is most likely a dead-end in my quest. Still, I learned a few things and I surely won\u0026rsquo;t remember any of it unless it\u0026rsquo;s writen down.\nThe plan I found a bunch of framebuffer terminal emulators, including fbcon, fbterm, bterm, yaft, and fbpad. I tried some of them (not all), and fbpad, by Ali Gholami Rudi, was the first to yield decent results, so it\u0026rsquo;ll be the focus of this article.\nHere is an overview of the steps:\n Download a suitable TTF font. Download and build ft2tf, and use it to convert the font to fbpad\u0026rsquo;s custom format. Download, configure, and build fbpad. Download Textual and run it in fbpad.  I\u0026rsquo;ll assume a Debian type of OS, like Raspberry Pi OS, Ubuntu, or a Debian distro.\nPreparing the font First, let\u0026rsquo;s download a suitable monospace font. I chose to use Fira Code.\ncd ~ wget -O fira.zip \u0026#34;https://fonts.google.com/download?family=Fira%20Code\u0026#34; unzip fira.zip -d fira Then, download and build ft2tf, a font conversion tool by the same author as fbpad:\ncd ~ apt-get install libfreetype-dev wget http://litcave.rudi.ir/ft2tf-0.9.tar.gz tar xzf ft2tf-0.9.tar.gz cd ft2tf-0.9 make Now we\u0026rsquo;re ready to convert the font:\n./ft2tf -h18 -w10 ~/fira/static/FiraCode-Regular.ttf:6 \u0026gt; ~/fira/fira.tf The -h and -w options specify the final glyph size in the terminal, which in turn determines the column and row count based on your screen resolution. The :6 part after the font path specifies the size at which the TTF font is scaled before conversion. I found these values to work decently well for my Raspberry Pi\u0026rsquo;s screen (a Waveshare 7\u0026quot; HDMI LCD), but YMMV.\nConfigure and build fbpad First, download fbpad:\ncd ~ git clone https://github.com/aligrudi/fbpad cd fbpad Then, edit conf.h and change these two options:\n// ...  #define TERM \u0026#34;xterm-256color\u0026#34; #define FR \u0026#34;/home/USERNAME/fira/fira.tf\u0026#34;  // ... Obviously, use your actual username. You can leave the rest unchanged.\nFinally, compile fbpad:\nmake Running Textual in fbpad First, grab a copy of Textual (here from source, using Poetry ‚Äì installing in a venv should work as well):\ncd ~ git clone https://github.com/Textualize/textual.git cd textual poetry install We\u0026rsquo;re finally ready to roll! Here is Textual color reference:\nTERMCOLOR=truecolor ~/fbpad/fbpad poetry run textual colors Note that TERMCOLOR=truecolor is required to have 256 colors, but it doesn\u0026rsquo;t actually enable 24bit true color mode, which fbpad does not support.\nHere is how it looks like in the Debian VM:\nThe Textual demo can ben run with the following command:\nTERMCOLOR=truecolor ~/fbpad/fbpad poetry run python -m textual Finally, here is a video of how it runs on the actual Raspberry Pi:\n   Final words As I mentioned before, this approach has many limitations:\n It\u0026rsquo;s obviously cumbersome to setup ‚Äì compiled configuration file anyone?? Hardly any of the framebuffer terminal emulators are active and well maintained projects. None of them have mouse support ‚Äì let alone handle a touch-screen1. The performance is bad, without an ounce of hardware acceleration.  For my purposes, I\u0026rsquo;ll likely move on to a bare-bone X11 setup without windows manager nor Desktop environment (basically startx + fullscreen xterm), since it addresses most of these limitations. Still, it was a fun niche to dig into! ü§ì\n  Extending Textual to support a touch-screen on Linux can be done with python-evdev\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://bylr.info/articles/2022/10/29/til-textual-fb-term/","summary":"My successful (yet probably vain) attempt at running Textual (a modern TUI framework) on a Raspberry Pi without X11 by using fbpad (a framebuffer terminal emulator for Linux).","title":"TIL: running Textual on a framebuffer terminal emulator for Linux"},{"content":"vpype 1.12 is out! üéâ\nNo ground-breaking features, but an improved \u0026ldquo;quality-of-life\u0026rdquo;, especially for Apple-silicon Mac owners, and few other goodies.\nLet\u0026rsquo;s dive in.\nMigration to PySide6  Migrated to PySide6 (from PySide2), which simplifies installation on Apple silicon Macs (#552, #559, #567)   PySide2 is the official Python wrapper for Qt 5, the GUI toolkit I use for the viewer. As Qt 5 doesn\u0026rsquo;t officially support Apple-silicon Macs, PySide2 ‚Äì and thus vpype until now ‚Äì were notoriously difficult to install on these computers. This is resolved with the transition to PySide6, which wraps Qt 6 and officially supports Apple-silicon Macs.\nThis took me waaaay too long. I actually feel bad for the struggle incurred to vpype users üòÖ All things considered, the migration wasn\u0026rsquo;t that complicated, but there were still a few pitfalls to figure out due to Qt 6 breaking changes around the OpenGL-based widget.\nMigrating to PySide6 is also a major step towards supporting Python 3.11, which brings a host of novelties as well as a significant performance bumps. I\u0026rsquo;m hoping this will happen by the next release, which means vpype 1.12 might well be the last to support Python 3.8.\nOther fixes and improvements  The layout command now properly handles the tight special case by fitting the page size around the existing geometries, accommodating for a margin if provided (#556) Fixed a viewer issue where page width/height of 0 would lead to errors and a blank display (#555)   Using layout tight would formerly set the page size to 0 by 0, which is useless in itself and caused a blank display. Not only the blanking issue has been resolved, but layout tight is now actually useful. It sets the page size to fit exactly the current geometries, accounting for a margin if --fit-to-margin MARGIN is provided.\n Added inch unit as a synonym to in, useful for expressions (in which in is a reserved keyword) (#541)   This addresses an oversight introduced with expressions in vpype 1.9. The units available for length CLI options are also available as scaling factor in expressions. For example, this creates a 10x15 cm rectangle:\nvpype rect 0 0 10cm \u0026#39;%15*cm%\u0026#39; show The expression works because the cm variable is made available by the interpreter, and set to the conversion factor between centimetres and pixels. This would however break for inches, because in is a reserved keyword in Python. The variable inch is now available instead. Either form can be used in CLI options, but inch must be used in expressions:\nvpype circle 5in 5inch \u0026#39;%3*inch%\u0026#39; show   Fixed a viewer issue where fitting the view to the document would not adjust when page size changes (vsketch only) (#564)   This change doesn\u0026rsquo;t directly benefits vpype as the page size cannot change while the viewer is active. In vsketch, however, the sketch code is free to set/change the page size based on GUI parameters, like in the included quick_draw example. In this case, when the view is fitted to the page size (i.e. as long as you don\u0026rsquo;t zoom or scroll), the view will adjust when the page size changes.\n Updated svgelements to 1.8.4, which fixes issue with some SVG constructs used by Matplotlib exports (#549)   Supporting all of the SVG standard subtleties is hard. Not only svgelements does a great job at it, but @tatarize\u0026rsquo;s reactivity when edge cases appear is unmatched. In this instance, Drawingbots' Discord user apur wanted to plot Matplotlib-generated SVGs of LaTeX equations. They included unusual \u0026lt;use\u0026gt; elements, which didn\u0026rsquo;t import properly. This is now fixed and I eagerly await the next niche corner case! ü§ó\n Migrated to Plausible.io (from Google Analytics) for vpype.readthedocs.io (#546)   Plausible is a privacy-focused, GDPR-compliant web statistics service. As I did for this site, I migrated from Google Analytics with my projects' documentation web sites. This is a paid service, so that neither your or I are the product.\nMystery changes  Added new units (yd, mi, and km) (#541) Added vpype.format_length() to convert pixel length into human-readable string with units (#541)   Yes, vpype supports kilometer-scale plots!1\nSeriously though, these changes are part of the WIP improvements of vpype\u0026rsquo;s terminal output. This will happen in future versions, but for some reason it was easier to integrate those change early.\nDeveloper-related changes  Poetry 1.2 or later is not required (developer only) (#541) A justfile is now provided for most common operations (install, build the documentation, etc.) (#541)   Shoutout to these two great dev tools: Poetry (Python dependency management) and just (make replacement for useful commands). I use them on a daily basis!\n  plotter not included\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  ","permalink":"https://bylr.info/articles/2022/10/25/annotated-release-notes-vpype-1.12/","summary":"\u003cp\u003e\u003cem\u003evpype\u003c/em\u003e 1.12 is out! üéâ\u003c/p\u003e\n\u003cp\u003eNo ground-breaking features, but an improved \u0026ldquo;quality-of-life\u0026rdquo;, especially for Apple-silicon Mac owners, and few other goodies.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s dive in.\u003c/p\u003e","title":"Annotated Release Notes: vpype 1.12"},{"content":"Although Google Analytics is very easy to setup on a Read the Docs-based documentation website, it requires a cookie banner to be GDPR-compliant and is otherwise questionable from a privacy-preservation point-of-view. As a result, I much prefer to use and support the excellent EU-based Plausible.io for traffic metrics instead.\nThis article explains how to setup a Read the Docs-based documentation with Plausible.io such that metrics are enabled only on \u0026ldquo;production\u0026rdquo; builds ‚Äî e.g. the \u0026ldquo;latest\u0026rdquo; builds from the main branch and the version-tagged builds. This minimises the contamination of traffic statistics by development-related activities.\nTo achieve this, the basic idea is to customise your Sphinx template such that the Plausible.io script is only included when a conf.py-defined flag is set to True. This flag is then set based on environment variables provided by Read the Docs.\nLet\u0026rsquo;s dive in the details a step at a time.\nEnabling templates If you haven\u0026rsquo;t done so already (for example to customise your API documentation), create a _templates sub-directory and let Sphinx know that this is where custom templates are to be found:\n# conf.py templates_path = [\u0026#34;_templates\u0026#34;] Customising the template Then, a custom template can be created to include the Plausible.io script. Create a _templates/base.html file with the following content:\n\u0026lt;!-- _templates/base.html --\u0026gt; {% extends \u0026#34;!base.html\u0026#34; %} {% block extrahead %} {% if enable_plausible %} \u0026lt;script defer data-domain=\u0026#34;myproject.readthedocs.io\u0026#34; src=\u0026#34;https://plausible.io/js/script.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {% endif %} {{ super() }} {% endblock %} In _templates/base.html, replace myproject.readthedocs.io by the actual domain name of your documentation. This domain name must also be enabled in your Plausible.io account.\nNote that the \u0026lt;script\u0026gt; tag is added only if the template variable enable_plausible evaluates to True. This is how we can control whether or not metrics should be enabled for a given build.\nImportant: I\u0026rsquo;m using the Furo theme, which uses base.html as main HTML file. Other themes (including the default Sphinx theme) might be using layout.html instead, as indicated in Sphinx\u0026rsquo;s documentation on templating. This initially threw me off, so make sure to check which of your template\u0026rsquo;s file must be extended.\nEnabling metrics on production build The enable_plausible variable must be defined for our template above to function. This is done in conf.py file using the html_context variable as follows:\n# conf.py import os # [...] READTHEDOCS_VERSION_TYPE = os.environ.get(\u0026#34;READTHEDOCS_VERSION_TYPE\u0026#34;, None) html_context = { \u0026#34;enable_plausible\u0026#34;: READTHEDOCS_VERSION_TYPE in [\u0026#34;branch\u0026#34;, \u0026#34;tag\u0026#34;], } I use the READTHEDOCS_VERSION_TYPE environment variable, which is set by Read the Docs. Its value is \u0026quot;branch\u0026quot; when the docs are built from the main branch, and \u0026quot;tag\u0026quot; when they are built from a tagged release. We want enable_plausible to be set to True in those instances. In any other case, including when READTHEDOCS_VERSION_TYPE is undefined (as is the case for local builds), enable_plausible is set to False.\nAnd that\u0026rsquo;s about it ‚Äì these few steps are all it takes more compliant and privacy-friendly metrics thanks to Plausible.io. For a real-world example, you can check my vpype project (relevant PR).\n","permalink":"https://bylr.info/articles/2022/10/09/til-plausible-rtd/","summary":"Although Google Analytics is very easy to setup on a Read the Docs-based documentation website, it requires a cookie banner to be GDPR-compliant and is otherwise questionable from a privacy-preservation point-of-view. As a result, I much prefer to use and support the excellent EU-based Plausible.io for traffic metrics instead.\nThis article explains how to setup a Read the Docs-based documentation with Plausible.io such that metrics are enabled only on \u0026ldquo;production\u0026rdquo; builds ‚Äî e.","title":"TIL: use Plausible.io with a Sphinx documentation hosted on RTD"},{"content":"Often, technical documentations include lists or other snippets of text that are strongly related to some of the project\u0026rsquo;s code. vpype\u0026rsquo;s documentation is no exception to this.\nFor instance, the Built-in symbols section lists the units available to expressions:\nThese units are related to the following piece of code:\n# vpype/utils.py UNITS = { \u0026#34;px\u0026#34;: 1.0, \u0026#34;in\u0026#34;: 96.0, \u0026#34;inch\u0026#34;: 96.0, \u0026#34;ft\u0026#34;: 12.0 * 96.0, \u0026#34;yd\u0026#34;: 36.0 * 96.0, \u0026#34;mi\u0026#34;: 1760.0 * 36.0 * 96.0, \u0026#34;mm\u0026#34;: 96.0 / 25.4, \u0026#34;cm\u0026#34;: 96.0 / 2.54, \u0026#34;m\u0026#34;: 100.0 * 96.0 / 2.54, \u0026#34;km\u0026#34;: 100_000.0 * 96.0 / 2.54, \u0026#34;pc\u0026#34;: 16.0, \u0026#34;pt\u0026#34;: 96.0 / 72.0, } I recently added support for more units and, of course, the documentation was at risk of running out of sync. Obviously, generating the list of units based on the code would be a better solution. After some Googling, here is how I did it.\nThe basic idea is to use substitutions. A substitution consists of assigning a text snippet to a keyword, and subsequently use said keyword (with the |keyword| syntax) in the documentation\u0026rsquo;s body. The second insight is to use the rst_prolog variable (within the conf.py file) for the definition. This being regular Python, the definition can easily be auto-generated based on the original code.\nHere is how it looks for the case above:\n# docs/conf.py import vpype as vp # [...] UNIT_STRINGS = \u0026#34;, \u0026#34;.join(f\u0026#34;``{s}``\u0026#34; for s in sorted(vp.UNITS.keys()) if s != \u0026#34;in\u0026#34;) rst_prolog = f\u0026#34;\u0026#34;\u0026#34; .. |units| replace:: {UNIT_STRINGS}\u0026#34;\u0026#34;\u0026#34; (Note that in is explicitly excluded from the list because it is a reserved Python keyword and cannot be used in the context of vpype expressions.)\nAnd this is how the substitution is used in the actual documentation file:\n.. docs/fundamentals.rst * Units constants (|units|). These variables may be used to convert values to CSS pixels unit, which *vpype* uses internally. For example, the expression ``%(3+4)*cm%`` evaluates to the pixel equivalent of 7 centimeters (e.g. ~264.6 pixels). (Note that expressions may overwrite these variables, e.g. to use the ``m`` variable for another purpose.)Et voil√†! Nice and easy. I certainly expect to use this technique often in the future.\n","permalink":"https://bylr.info/articles/2022/09/30/til-sphinx-substitutions/","summary":"Often, technical documentations include lists or other snippets of text that are strongly related to some of the project\u0026rsquo;s code. vpype\u0026rsquo;s documentation is no exception to this.\nFor instance, the Built-in symbols section lists the units available to expressions:\nThese units are related to the following piece of code:\n# vpype/utils.py UNITS = { \u0026#34;px\u0026#34;: 1.0, \u0026#34;in\u0026#34;: 96.0, \u0026#34;inch\u0026#34;: 96.0, \u0026#34;ft\u0026#34;: 12.0 * 96.0, \u0026#34;yd\u0026#34;: 36.0 * 96.0, \u0026#34;mi\u0026#34;: 1760.0 * 36.","title":"TIL: using Sphinx substitutions to generate text snippets from code"},{"content":"This release further solidifies the block commands which were overhauled in vpype 1.9. It also introduces several changes revolving around the \u0026ldquo;plotting with paint\u0026rdquo; use-case, which typically requires the brush to be regularly dipped in a paint well. This can be achieved by inserting \u0026ldquo;dipping\u0026rdquo; patterns at regular intervals determined by the cumulative drawing distance. vpype 1.11 makes this process much easier.\nThanks a lot to Andee Collard for his useful feedback and providing this article\u0026rsquo;s banner!\nPainting with a plotter  Added the splitdist command to split layers by drawing distance (thanks to @LoicGoulefert) (#487, #501)   The new splitdist command, contributed by Lo√Øc Goulefert (thanks a lot!), is the core of the paint plotting use-case. It splits each layer into newly created layers such that their respective drawing distance is each below the specified limit.\nThis command could readily be used with a clever vpype-gcode profile that implements the dipping mechanism at the beginning of each layer. Alternatively, it can be combined with the forlayer block command to insert dipping patterns into the line work. We\u0026rsquo;ll see an example of such a pipeline below.\n Added meters (m) and feet (ft) to the supported units (#498, #508) Fixed an issue with expressions where some variable names corresponding to units (e.g. m) could not be used (expressions may now reuse these names) (#506)   These are rather large units for typical plotting workflow, but come in useful for specifying the maximum drawing distance with splitdist.\nAs a reminder, units are available in two contexts:\n Every time a command accepts a length-type argument or option (e.g. translate 5mm 3cm or linemerge --tolerance 0.05mm). In expressions (e.g. forlayer translate \u0026quot;%_i*3*cm%\u0026quot; 0 end).  In the latter case, the existence of the unit constant precluded the use of variables with the same name. This issue worsened with the addition of m as this is a rather common variable name (e.g. this cookbook recipe uses it). To address this, they are no longer read-only and may now be overwritten. Of course, doing so renders their original value unavailable in the pipeline\u0026rsquo;s subsequent expressions.\n Fixed an issue with blocks where certain nested commands could lead totally unexpected results (#506) API: removed the faulty temp_document() context manager from vpype_cli.State() (#506)   The improved blocks introduced in vpype 1.9 had a major flaw which could, in some circumstances, result in erratic results. It turns out that the new splitdist command triggered this issue and brought it in the spotlight. This is now fixed, and the vpype_cli.State.temp_document() API is a casualty of this patch (luckily, it was introduced recently and I\u0026rsquo;m pretty sure no one used it yet besides me).\n Fixed an issue with the lmove command where order would not be respected in certain cases such as lmove all 2 (the content of layer 2 was placed before that of layer 1) (#506)   This is yet another issue highlighted by to the \u0026ldquo;plotting with paint\u0026rdquo; workflow. When the source layers included the destination layer (as is the case for lmove all 2), the order of the source layers would not be respected (e.g. for a 3-layer pipeline and thelmove all 2 command, layer 2 would end up with its original content, then layer 1, then layer 3). With this fix, the destination layer will now include the source layers' content in the correct order (e.g. in the previous example, layer 2 would end up with the content of layer 1, then layer 2, then layer 3).\n Collectively, these changes enable the \u0026ldquo;plotting with paint\u0026rdquo; workflow using the following pipeline:\n$ vpype \\  read input.svg \\  forlayer \\  lmove %_lid% 1 \\  splitdist 1m \\  forlayer \\  lmove %_lid% \u0026#34;%_lid*2%\u0026#34; \\  read -l \u0026#34;%_lid*2-1%\u0026#34; dip_%_name%.svg \\  end \\  lmove all %_lid% \\  name -l %_lid% %_name% \\  color -l %_lid% %_color% \\  end \\  write output.svg For this to work, the layers in input.svg must be named after their respective color and, for each such color, a file named dip_COLORNAME.svg must exist. For example, if input.svg has two layers named \u0026ldquo;red\u0026rdquo; and \u0026ldquo;blue\u0026rdquo;, then the dip_red.svg and dip_blue.svg files must exist.\nThe following figure illustrates the results for synthetic data.\n (left) Input SVG with 3 layers. (middle) The three corresponding dipping pattern SVGs. (right) The output SVG with 3 layers and the visible dipping patterns interspersed within the line work.\n  This pipeline is listed in a cookbook recipe and will be explained in details, along with the forlayer block command, in a future article.\nOther changes  Improved the linemerge algorithm by making it less dependent on line order (#496)   The linemerge command is implemented using a greedy algorithm which roughly works as follows:\n Pick the first available line. Look for another line that can be appended. If found, merge both lines and look for further line to append (back to step 2). If not, save the current line, pick the next available one, and repeat (back to step 1).  By default, linemerge always considers both endings of each line, possibly reversing them if this enables a merge. This is not always desirable though, which is why the --no-flip option exists. In this case, the algorithm would only try to append to the current line, without trying to prepend as well. This oversight led to a greater dependence on line order and, occasionally, suboptimal results, as illustrated by the figure below.\n  (left) Initial situation. (middle) Result when both appending only. (right) Results when appending and prepending.\n  With this fix, linemerge --no-flip now tries to both append and prepend, leading to more consistent results.\n Added --keep-page-size option to grid command (#506)   By default, the grid block command sets the page size to its geometry. For example, the block grid --offset 4cm 3cm 3 5 [...] end sets the page size to 12x15cm. This behaviour can now be disabled with the --keep-page-size option.\nThis change mainly helps for the testability of the blocks feature (in this release, I\u0026rsquo;ve added multiple tests to minimise the risk of future regression), but I figured it could have its occasional use out there.\n Added HPGL configurations for the Houston Instrument DMP-161, HP7550, Roland DXY 1xxxseries and sketchmate plotters (thanks to @jimmykl and @ithinkido) (#472, #474)   Thanks a lot, Jimmy Kirkus-Lamont and @ithinkido! ‚ù§Ô∏è\n Added equality operator to vpype.LineCollection and vpype.Document (#506)   I can now check if two layers or documents have the same content and metadata using the equality operator ==. This is immensely useful when writing tests. I have no idea why it took so long‚Ä¶ ü§∑\n Pinned Shapely to 1.8.2, which is the first release in a long time to have binaries for most platforms/Python release combination (including Apple-silicon Macs and Python 3.10) (#475)   It was quite the roller coaster ride for Shapely to be properly packaged for both Python 3.10 and Apple-silicon Macs, but now this is fully sorted out. That\u0026rsquo;s one less hassle when installing vpype.\n Removed deprecated API (#507)   With vpype 1.9, a number of APIs migrated from the vpype package to the vpype_cli package. The former APIs still worked but emitted deprecation warnings. They are now gone forever.\n","permalink":"https://bylr.info/articles/2022/07/06/annotated-release-notes-vpype-1.11/","summary":"This release further solidifies the block commands which were overhauled in vpype 1.9. It also introduces several changes revolving around the \u0026ldquo;plotting with paint\u0026rdquo; use-case, which typically requires the brush to be regularly dipped in a paint well. This can be achieved by inserting \u0026ldquo;dipping\u0026rdquo; patterns at regular intervals determined by the cumulative drawing distance. vpype 1.11 makes this process much easier.\nThanks a lot to Andee Collard for his useful feedback and providing this article\u0026rsquo;s banner!","title":"Annotated Release Notes: vpype 1.11"},{"content":"Following a recent discussion on Twitter, I decided to take yet another deep dive in my Python projects' documentation and fix once and for all the issues I had with it. I first focused on the automatically-generated API reference section and this article details the results of my finding. Specifically, I\u0026rsquo;m using vsketch\u0026rsquo;s API reference, which I recently updated, as an example (documentation source.\nThis article addresses the following objectives:\n Produce a beautiful API documentation based on the code docstrings that is both nice to look at and easy to navigate. Support for a proper table of content navigation down to each class/module\u0026rsquo;s member. Nice looking summary tables listing modules' and classes' contents.  This article targets an audience of FOSS maintainers who are unhappy with the state of their project\u0026rsquo;s API documentation, and are frustrated with the process of improving it. A basic understanding of Sphinx as well as an existing documentation setup is assumed. This article basically contains everything I wish I was told when I first started on my API reference improvement journey. For the beginners, I\u0026rsquo;ll provide pointers to help setting up a basic Sphinx.\nNote that although this article is structured as a tutorial, it covers tips and techniques which are likely useful for other kinds of documentation customisation.\nBasic setup As stated above, the basic steps to setup a Sphinx-based documentation project are outside the scope of the present article. I suggest reviewing the following resources to get started:\n @Mariatta made a brilliant tutorial on how to kick-start a Sphinx documentation project. Thomas Cokelaer has a very nice reStructuredText cheat sheet. Simon Willison wrote another cheat sheet which covers \u0026ldquo;the subset of reStructuredText worth committing to memory\u0026rdquo;. Obviously, Sphinx\u0026rsquo;s documentation is an important resource. Although it is somewhat arid for the newcomer, I strongly suggest not giving up on it. I had multiple \u0026ldquo;oh there it is!\u0026rdquo; moments with it in the process of writing this article. Finally, Read the Docs is likely the best place to host your documentation. It\u0026rsquo;s very simple to setup and free for open source projects.  As for the theme, my preference goes for Pradyun Gedam\u0026rsquo;s Furo. I\u0026rsquo;m using it with the default configuration, so the only requirement is to enable it in your conf.py file:\nhtml_theme = \u0026#34;furo\u0026#34; Note that some of the CSS provided in this article may need adjustments should you opt for a different theme.\nAutoapi setup After trying both autodoc/autosummary and Sphinx AutoAPI, I opted to use the latter. Here are the main reasons behind this choice:\n Autosummary does not generate TOC entries for API elements such as classes/modules and their members. This is due to a long-standing Sphinx limitation. Autoapi works around this limitation (albeit imperfectly, as we\u0026rsquo;ll later note). Autosummary defaults to not generating anything and is in my experience frustrating to setup. In contrast, autoapi produces usable output out-of-the-box. Templates are easier to write thanks to the rich \u0026ldquo;mapper\u0026rdquo; objects AutoAPI provides after parsing your code (see the AutoAPI objects section below).  Note that there are two things called \u0026ldquo;autoapi\u0026rdquo; floating on the Internet: the Sphinx AutoAPI project (documentation) is the good one. The other one is unmaintained and barely documented. Make sure you don\u0026rsquo;t loose time dealing with the wrong one.\nBasics Setting up Sphinx AutoAPI is covered in their documentation. It boils down to the following steps.\n Install the sphinx-autoapi package: $ pip install sphinx-autoapi  Add AutoAPI it to the Sphinx extension list: extensions = [ ..., \u0026#39;autoapi.extension\u0026#39;, ]  List your package directories (or the directory containing them) and set basic options: autoapi_dirs = [\u0026#39;../mypackage\u0026#39;] autoapi_type = \u0026#34;python\u0026#34;  Add the generated documentation to your index.rst toctree: .. toctree:: :maxdepth: 3... autoapi/index   Setting up templates We will customise Sphinx AutoAPI\u0026rsquo;s default templates. The easiest is to copy Sphinx AutoAPI\u0026rsquo;s default templates in your project to serve as a starting point.\nFirst, run the following commands (adjusting for your Python version) from your documentation directory:\n$ mkdir _templates $ mkdir _template/autoapi $ cp $VIRTUAL_ENV/lib/python3.10/site-packages/autoapi/templates/index.rst _templates/autoapi/ $ cp -r $VIRTUAL_ENV/lib/python3.10/site-packages/autoapi/templates/python _templates/autoapi/ Then, tell Sphinx AutoAPI of its template directory in your conf.py file:\nautoapi_template_dir = \u0026#34;_templates/autoapi\u0026#34; A useful tip is to make a Git commit just after copying the built-in templates, such that you can track (and revert) your modifications. I\u0026rsquo;ve used this extensively while working on my templates.\nAt this point, I suggest spending some time to become acquainted with the built-in templates and how they are organised and implemented. If you haven\u0026rsquo;t used it before, it is also useful to review the Jinja2 templating language documentation.\nOther configuration options autoapi_options The autoapi_options controls various aspect of the generated documentation, including the type of class/module members that are listed. Its default value is sensible but I still felt like customising it:\nautoapi_options = [ \u0026#34;members\u0026#34;, \u0026#34;undoc-members\u0026#34;, \u0026#34;show-inheritance\u0026#34;, \u0026#34;show-module-summary\u0026#34;, \u0026#34;imported-members\u0026#34;, ] In particular, I want the summary at the top of the module\u0026rsquo;s documentation (show-module-summary), but we will heavily customise it. Check the documentation for a list of available options and their descriptions.\nautoapi_keep_files Another useful option is autoapi_keep_files. Sphinx-autoapi generates .rst pages for the documentation during the build process, but defaults to deleting them after completion. It\u0026rsquo;s often useful to keep them around for inspection and debugging purposes:\nautoapi_keep_files = True autodoc_typehints This is technically an autodoc setting, but Sphinx AutoAPI honours it. It controls if/where type hints are included in the documentation. The possible values are the following:\n \u0026quot;signature\u0026quot;: type hints are included in the function signature, which appears first in the member\u0026rsquo;s documentation \u0026quot;description\u0026quot;: type hints are included within the function description, when the arguments are listed \u0026quot;both\u0026quot;: type hints are included in both places \u0026quot;none\u0026quot;: type hints are not included  My preference goes for \u0026quot;signature\u0026quot;:\nautodoc_typehints = \u0026#34;signature\u0026#34; AutoAPI objects Understanding the Sphinx AutoAPI objects is key to customising templates. They are one of the major difference with respect to autodoc/autosummary.\nIn order to generate the API documentation, the autodoc loads your actual code and uses Python\u0026rsquo;s introspection capabilities to extract the required information from your module and class objects. In contrast, Sphinx AutoAPI parses your Python code and builds a collection of so-called \u0026ldquo;mapper\u0026rdquo; objects which describe your code and its structure. These objects are then passed on as context to the Jinja2 templating engine. Oddly, the documentation doesn\u0026rsquo;t provide a reference about them, but their implementation is easy to read.\nHere is a summary of some of the attributes that are useful when writing templates:\n obj.name Name of the mapped object, e.g. \u0026quot;MyClass\u0026quot; or \u0026quot;my_method\u0026quot;. obj.id Fully qualified name of the object, used for cross-referencing, e.g. \u0026quot;my_module.MyClass\u0026quot; or \u0026quot;my_module.MyClass.my_method\u0026quot;. obj.summary Summary of the object\u0026rsquo;s docstring (i.e. the first line). obj.docstring Full docstring of the object. obj.display Indicates whether or not this object should be displayed, based on the options set in conf.py and the result of the autoapi-skip-member event (discussed later). obj.children (Modules and classes only) List children functions, methods, attributes, etc. obj.properties (Functions and methods only) List of properties, such as \u0026quot;classmethod\u0026quot;, \u0026quot;staticmethod\u0026quot;\u0026quot;, \u0026quot;abstractmethod\u0026quot;, \u0026quot;property\u0026quot;, etc. obj.obj.args (Functions and methods only) List of 4-tuples describing the function\u0026rsquo;s arguments. The first item is the star operator if any (\u0026quot;*\u0026quot;, \u0026quot;**\u0026quot;, or None), the second is the argument name, the third is the argument type or None, and the fourth is the argument default value or None. This key piece of data will enable us to recreate the signatures according to our needs.  When working on your documentation, it is often useful to inspect the contents of these mapper objects using a debugger. This can be achieved by adding an handler for the autoapi-skip-member event and setting a conditional breakpoint:\ndef skip_member(app, what, name, obj, skip, options): # conditional breakpoint here return skip def setup(sphinx): sphinx.connect(\u0026#34;autoapi-skip-member\u0026#34;, skip_member) This event will be triggered for every single Python object parsed from your code. By breaking, for example, when obj.name == \u0026quot;my_module\u0026quot;, the obj argument and its children can be fully inspected. I use the following run configuration in my IDE for this:\n Execute module: sphinx.cmd.build Parameters: -M html . _build Working directory: docs/  An autosummary-like macro By default, Sphinx AutoAPI provides a summary list of classes, functions, and attributes at the top of a module\u0026rsquo;s documentation, which is very nice. Our objective is to add a similar table at the top of each class description, to facilitate navigation. However, Sphinx AutoAPI uses its own autoapisummary directive, which derives from autosummary\u0026rsquo;s autosummary directive. Both suffer from the following limitations:\n The way callables are rendered is hard-coded and cannot be customised via templates. In particular, if autodoc_typehints is set to \u0026quot;signature\u0026quot; or \u0026quot;both\u0026quot;, autosummary will include type hints in the summary table as well. Unfortunately, this dramatically increases the length of the signature, which is detrimental to the table layout and usability. Alternatively, signatures can be entirely removed by using the :nosignatures: option. However, in this case, not even parenthesis are displayed, which hides the callable nature of the function. The best compromise is to have the full signature with their arguments, but without typing annotations. Properties are listed as functions, including their signature. This hides the fact that, API-wise, they behave as data members (though it would still be useful to indicate that they are in fact properties). There is not indication that a method is abstract, static, or class-based.  To address these shortcomings, we will create a Jinja2 template macro to replicate and improve on autosummary/autoapisummary functionality.1\nOur aim is to create tables where callable have their full ‚Äì but unannotated ‚Äì signature, where properties are indicated as such but rendered as attributes, and where static, class, and abstract methods are marked as such. Here is an example of this:\nBasic macro setup The basic insight is that a summary table can be implemented using Sphinx\u0026rsquo;s list-table:\n.. list-table:: Title :header-rows: 0 :widths: auto * - Item 1 - This is the description of the first item. * - Name 2 - This is also a description, but this time for the second item. * - ... - ...Such a table can be generated with the following Jinja macro:\n{% macro auto_summary(objs, title=\u0026#39;\u0026#39;) -%} .. list-table:: {{ title }} :header-rows: 0 :widths: auto {% for obj in objs %} * - obj.name - obj.summary {% endfor %} {% endmacro %} To test this, create a file named _templates/autoapi/macros.rst and add the code above. Then, make the following edits to the _templates/autoapi/python/module.rst file:\n At the top of the file, import macros.rst to make it available for use: {% import \u0026#39;macros.rst\u0026#39; as macros %}  Locate where the class summary is generated: .. autoapisummary:: {% for klass in visible_classes %} {{ klass.id }} {% endfor %}  Replace the code above by a call to our macro: {{ macros.auto_summary(visible_classes, title=\u0026#34;Classes\u0026#34;) }}   Here is the result I obtain with my project:\nThis is a good start, but we\u0026rsquo;re obviously far from the result we want. To start with, no cross-reference links are generated. And, had we passed functions or methods instead of classes to our macro, no signature would have been generated.\nCustom labels Before fixing our macro, we must discuss these nice looking \u0026ldquo;prop\u0026rdquo;, \u0026ldquo;static\u0026rdquo;, and \u0026ldquo;class\u0026rdquo; tag-like labels in the example tables above. These are implemented using a custom role with some CSS attached to it.\nThis StackOverflow answer explains how to create a custom role and make it globally available to your documentation. Basically, just add the following to your conf.py file:\nrst_prolog = \u0026#34;\u0026#34;\u0026#34; .. role:: summarylabel \u0026#34;\u0026#34;\u0026#34; The role directive creates a new role which can then be used as follows:\n:summarylabel:`My Label`Sphinx generates the corresponding HTML code:\n\u0026lt;span class=\u0026#34;summarylabel\u0026#34;\u0026gt;My label\u0026lt;/span\u0026gt; Since it sets an HTML class named after the role, it\u0026rsquo;s easy to adjust the label appearance using some custom CSS. Create a file named _static/css/custom.css in your documentation directory and add the following CSS:\nspan.summarylabel { background-color: var(--color-foreground-secondary); color: var(--color-background-secondary); font-size: 70%; padding-left: 2px; padding-right: 2px; border-radius: 3px; vertical-align: 15%; padding-bottom: 2px; filter: opacity(40%); } Note the use of CSS variables in order to support Furo\u0026rsquo;s dynamic night mode feature.\nFinally, we must tell Sphinx about this CSS file in the conf.py file:\nhtml_css_files = [ \u0026#34;css/custom.css\u0026#34;, ] Customising the table appearance A similar CSS approach can be used to customise the appearance of the summary table itself. By adding the :class: option to the list-table directive, we can tell Sphinx to attach an HTML class to the \u0026lt;table\u0026gt; element, which we can then customise with CSS:\n.. list-table:: Title :header-rows: 0 :widths: auto :class: summarytable * - ... - ...For my project, the only change I made to the default appearance is to force the table to span the entire width regardless of its contents. This can be done by adding the following code to our custom.css file:\ntable.summarytable { width: 100%; } Putting it all together We are now ready to put everything together and improve our auto_summary() macro to our liking. Here is the final code for macros.rst:\n{% macro _render_item_name(obj, sig=False) -%} :py:obj:`{{ obj.name }} \u0026lt;{{ obj.id }}\u0026gt;` {%- if sig -%} \\ ( {%- for arg in obj.obj.args -%} {%- if arg[0] %}{{ arg[0]|replace(\u0026#39;*\u0026#39;, \u0026#39;\\*\u0026#39;) }}{% endif -%}{{ arg[1] -}} {%- if not loop.last %}, {% endif -%} {%- endfor -%} ){%- endif -%} {%- endmacro %} {% macro _item(obj, sig=False, label=\u0026#39;\u0026#39;) %} * - {{ _render_item_name(obj, sig) }} - {% if label %}:summarylabel:`{{ label }}` {% endif %}{% if obj.summary %}{{ obj.summary }}{% else %}\\-{% endif +%} {% endmacro %} {% macro auto_summary(objs, title=\u0026#39;\u0026#39;) -%} .. list-table:: {{ title }} :header-rows: 0 :widths: auto :class: summarytable {% for obj in objs -%} {%- set sig = (obj.type in [\u0026#39;method\u0026#39;, \u0026#39;function\u0026#39;] and not \u0026#39;property\u0026#39; in obj.properties) -%} {%- if \u0026#39;property\u0026#39; in obj.properties -%} {%- set label = \u0026#39;prop\u0026#39; -%} {%- elif \u0026#39;classmethod\u0026#39; in obj.properties -%} {%- set label = \u0026#39;class\u0026#39; -%} {%- elif \u0026#39;abstractmethod\u0026#39; in obj.properties -%} {%- set label = \u0026#39;abc\u0026#39; -%} {%- elif \u0026#39;staticmethod\u0026#39; in obj.properties -%} {%- set label = \u0026#39;static\u0026#39; -%} {%- else -%} {%- set label = \u0026#39;\u0026#39; -%} {%- endif -%} {{- _item(obj, sig=sig, label=label) -}} {%- endfor -%} {% endmacro %} The work is now split in three macros:\n auto_summary() This is the \u0026ldquo;public\u0026rdquo; macro. It generates a table based on a list of mapper objects, with an optional title. It iterates over the list of objects, and, for each of them, determines if the signature should be generated (functions and non-property methods) and if some label should be attached. It then uses _item() to generate each object\u0026rsquo;s code. _item() This helper macro generates the code for each object, prepending a label to the summary if requested. _render_item_name() This helper macro focuses on generating the properly-cross-referenced object name. If the signature is requested, it iterates over the obj.obj.args list to produce a full (but unannotated) list of arguments.  Improving the default templates With our auto_summary() macro completed, we are now ready to customise our templates, but we still have one issue to resolve before we do so.\nCategorising objects with a custom Jinja2 test As we saw in the AutoAPI objects section, mapper objects representing modules or classes have a children attribute which lists the objects it contains. For example, a module\u0026rsquo;s children attribute lists all the classes, functions and attributes defined within it.\nIn order to categorise these children into separate sub-lists, the built-in templates heavily use the selectattr() and rejectattr() filters. For example, a list of classes in a module can be obtained as follows:\n{% set visible_children = module_object.children|selectattr(\u0026#34;display\u0026#34;)|rejectattr(\u0026#34;imported\u0026#34;)|list %} {% set visible_classes = visible_children|selectattr(\u0026#34;type\u0026#34;, \u0026#34;equalto\u0026#34;, \u0026#34;class\u0026#34;)|list %} This code selects the visible (but not imported) children from module_object, and then further selects children which have their type set to \u0026quot;class\u0026quot;. In the code above, \u0026quot;equalto\u0026quot; is known as a Jinja test. There are many such built-in tests in Jinja2.\nAs stated before, we aim to categorise properties as attributes instead of methods. To that end, we will have to filter methods whose properties attribute contains \u0026quot;property\u0026quot;. Intuition dictates that the following code achieves this:\n{% set property_methods = all_methods|selectattr(\u0026#34;properties\u0026#34;, \u0026#34;contains\u0026#34;, \u0026#34;property\u0026#34;)|list %} The bad news is that no such \u0026quot;contains\u0026quot; test exists by default in Jinja2. The good news is that it is trivial to add one.\nFirst, the actual test must be written. Add the following code to your conf.py file:\ndef contains(seq, item): return item in seq Then, we just need to add this test to the Jinja environment. Sphinx AutoAPI provides a hook for that:\ndef prepare_jinja_env(jinja_env) -\u0026gt; None: jinja_env.tests[\u0026#34;contains\u0026#34;] = contains autoapi_prepare_jinja_env = prepare_jinja_env With this in your conf.py file, the template code above will work as expected.\nUpdating templates We previously replaced one of module.rst\u0026rsquo;s use of autoapisummary by our auto_summary() macro (see Basic macro setup). It is now time to generalise the use of our macro. At this stage, the details of how this is done is to a large extent up to reader\u0026rsquo;s taste. The templates of vsketch can serve as fully-functional example and can readily be used in another projects.\nFor the module.rst template, I have opted to simplify the overview\u0026rsquo;s structure by just generating tables (without headings) for classes, functions, and attributes:\n{% if \u0026#34;show-module-summary\u0026#34; in autoapi_options and (visible_classes or visible_functions) %} {% block classes scoped %} {% if visible_classes %} {{ macros.auto_summary(visible_classes, title=\u0026#34;Classes\u0026#34;) }} {% endif %} {% endblock %} {% block functions scoped %} {% if visible_functions %} {{ macros.auto_summary(visible_functions, title=\u0026#34;Functions\u0026#34;) }} {% endif %} {% endblock %} {% block attributes scoped %} {% if visible_attributes %} {{ macros.auto_summary(visible_attributes, title=\u0026#34;Attributes\u0026#34;) }} {% endif %} {% endblock %} {% endif %} For the class.rst template, I chose to rework the structure of the documentation into two rubrics:\n{% if visible_methods or visible_attributes %} .. rubric:: Overview {% set summary_methods = visible_methods|rejectattr(\u0026#34;properties\u0026#34;, \u0026#34;contains\u0026#34;, \u0026#34;property\u0026#34;)|list %} {% set summary_attributes = visible_attributes + visible_methods|selectattr(\u0026#34;properties\u0026#34;, \u0026#34;contains\u0026#34;, \u0026#34;property\u0026#34;)|list %} {% if summary_attributes %} {{ macros.auto_summary(summary_attributes, title=\u0026#34;Attributes\u0026#34;)|indent(3) }} {% endif %} {% if summary_methods %} {{ macros.auto_summary(summary_methods, title=\u0026#34;Methods\u0026#34;)|indent(3) }} {% endif %} .. rubric:: Members {% for attribute in visible_attributes %} {{ attribute.render()|indent(3) }} {% endfor %} {% for method in visible_methods %} {{ method.render()|indent(3) }} {% endfor %} {% endif %} Note the use of the custom \u0026quot;contains\u0026quot; Jinja2 test we implemented earlier.\nHiding submodules Sphinx AutoAPI include all subpackages and submodules recursively, unless those are marked as private by prefixing their name with an underscore. In my packages' __init__.py file, I carefully import from submodules the objects which are meant to be public, but haven\u0026rsquo;t necessarily marked the submodules as private. Sphinx AutoAPI has no option to control whether or not to add them (I suggested adding one), so I had to filter them out manually. This is done with the autoapi-skip-member event handler we mentioned earlier:\ndef skip_member(app, what, name, obj, skip, options): # skip submodules if what == \u0026#34;module\u0026#34;: skip = True return skip def setup(sphinx): sphinx.connect(\u0026#34;autoapi-skip-member\u0026#34;, skip_member) Hiding members Likewise, it may happen that you want to hide specific members from the documentation without marking them as private. Again, the autoapi-skip-member event handler can do that. The following example is based from actual code in vsketch:\ndef skip_member(app, what, name, obj, skip, options): if \u0026#34;vsketch.SketchClass\u0026#34; in name: if obj.name in [ \u0026#34;vsk\u0026#34;, \u0026#34;param_set\u0026#34;, \u0026#34;execute_draw\u0026#34;, \u0026#34;ensure_finalized\u0026#34;, \u0026#34;execute\u0026#34;, \u0026#34;get_params\u0026#34;, \u0026#34;set_param_set\u0026#34;, ]: skip = True return skip Note that name is the fully qualified name of the object, so the vsk member has name set to vsketch.SketchClass.vsk. In contrast, obj.name is just the base name.\nAlso, for this to work as expected for modules, I had to change the following line in module.rst\n{% set visible_children = obj.children|selectattr(\u0026#34;short_name\u0026#34;, \u0026#34;in\u0026#34;, obj.all)|list %} into\n{% set visible_children = obj.children|selectattr(\u0026#34;display\u0026#34;)|selectattr(\u0026#34;short_name\u0026#34;, \u0026#34;in\u0026#34;, obj.all)|list %} Without this modification, objects marked as skipped would show up in the summary tables.\nOrdering By default, Sphinx AutoAPI generates the documentation in the same order as the code. This can be changed to alphabetical order, I like being in control from the code.\nIn vsketch, the top level content is determined by the imports in my package\u0026rsquo;s __init__.py file, so the import statements themself matter. Since I\u0026rsquo;m using isort, I had to short-circuit it in this particular case:\n# vsketch/__init__.py # isort: skip_file # Ordered for the documentation from .vsketch import Vsketch from .shape import Shape from .sketch_class import SketchClass, Param, ParamType from .easing import EASING_FUNCTIONS from .utils import working_directory Conclusion Well, this ended up being much longer than anticipated üòÖ ‚Äì if you made it this far, congratulations! üéâ\nHere is what we covered:\n We built on a basic Sphinx project and added AutoAPI to generate an API reference documentation. We created custom templates based on the built-in templates provided by AutoAPI. We reviewed the mapper objects created by AutoAPI to described our code. We learned how to use a debugger to easily inspect these objects. We crafted an autosummary-like macro named auto_summary() to build beautiful summary tables. We customised these tables with custom CSS. We used a custom Sphinx role and CSS to create tag-like labels to be used in the summary tables. We learned of Jinja2\u0026rsquo;s tests and created a custom one. We controlled the visibility of submodules and members using a autoapi-skip-member event handler. We learned how to control ordering from our code.  Like any software project, improving the documentation is a never-ending endeavour. As it turns out, there is one remaining issue that has been bugging me and is yet unresolved. Due to a limitation in Sphinx, AutoAPI has a tendency to mangle the TOC ordering, especially when section headings are emitted from the templates. Check these two issues for more information. Hopefully they\u0026rsquo;ll get solved in the future.\nI shall conclude by stating that, by all means, I do not consider myself a Sphinx expert ‚Äî much to the contrary. I did spend a lot of time improving my API documentation, and figured it would be wasteful not to share my findings, especially given the relative scarcity of information on advanced Sphinx usage patterns. As a result, it is rather likely that I made mistakes and sub-optimal choices. If so, please do provide feedback, and I\u0026rsquo;ll update this article to improve it.\nEdit: updated title and intro to clarify the nature of the API discussed, i.e. Python API (2022-05-11).\n  Ideally, these shortcomings would be addressed using an extension and a custom directive, or even by contributing the improvement back to the Sphinx or AutoAPI projects. This is sadly beyond my skills. Also, the template method is anyway useful for highly specific requirements where writing an extension wouldn\u0026rsquo;t be warranted.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://bylr.info/articles/2022/05/10/api-doc-with-sphinx-autoapi/","summary":"Following a recent discussion on Twitter, I decided to take yet another deep dive in my Python projects' documentation and fix once and for all the issues I had with it. I first focused on the automatically-generated API reference section and this article details the results of my finding. Specifically, I\u0026rsquo;m using vsketch\u0026rsquo;s API reference, which I recently updated, as an example (documentation source.\nThis article addresses the following objectives:","title":"Generating beautiful Python API documentation with Sphinx AutoAPI"},{"content":"Hatch fills or pixel art plotting requires a rather precise estimate of your particular pen/paper combo\u0026rsquo;s stroke width.\nFor example, this test pixel art plot would benefit from a slightly thinner pitch to avoid the visible overlap between neighbouring lines:\nThere is no way around experience to find the optimal pitch. I\u0026rsquo;ve created the fill_test sketch to create custom charts with test patterns precisely tuned to the pen of interest. There is indeed no point to testing a rotring isograph .35mm pen with a generic 0.1mm to 1.0mm chart.\nHere is how the UI looks:\nBeyond the general layout options (page size, grid size, etc.), the Smallest Width and Width Increment parameters enable a fine-grained exploration of pitches around the nominal pen size.\nHere is an example with a rotring isograph .35mm in my notebook, which has a slight propensity for ink soaking. For this combo, fills with .4mm pitch yield the best results:\nTo use the sketch, download or clone my sketches repository and execute the sketch using your existing vsketch installation:\n$ vsk run path/to/sketchs/fill_test ","permalink":"https://bylr.info/articles/2022/04/28/sketch-fill-test/","summary":"Hatch fills or pixel art plotting requires a rather precise estimate of your particular pen/paper combo\u0026rsquo;s stroke width.\nFor example, this test pixel art plot would benefit from a slightly thinner pitch to avoid the visible overlap between neighbouring lines:\nThere is no way around experience to find the optimal pitch. I\u0026rsquo;ve created the fill_test sketch to create custom charts with test patterns precisely tuned to the pen of interest. There is indeed no point to testing a rotring isograph .","title":"Sketch: fill test pattern generator"},{"content":"The problem Several generative art algorithms, such as Truchet tiles, use a regular grid of square cells. For example, check this interactive demo from the Generative Design book or these few pieces of mine.\nNow, let\u0026rsquo;s say you want to generate an iteration of your algorithm for printing or plotting such that all margins around the grid are the same for the given paper size. You can of course adjust the number of cell rows and columns, but how should you size the cell such as to achieve uniform margins?\nThe image above illustrates the problem. For a given page of size $W \\times H$ and a regular grid of $N \\times M$ cells, what should be the cell size $s$ to achieve uniform margins $m$ around the grid? What is then the value of $m$?\nThe solution This is easy to solve with a bit of math. Here is the system of two equations that must be solved:\n \\[\\begin{cases} 2 \\cdot m \u0026#43; N \\cdot s = W \\\\ 2 \\cdot m \u0026#43; M \\cdot s = H \\end{cases}\\]  with all parameters ($N$, $M$, $W$, $H$) and the cell size $s$ being strictly positive.\nSolving this for $m$ and $s$ is no rocket science but Wolfram Alpha can do the job for you if your high school math is rusty!\nWe have the following solutions:\n \\[\\begin{cases} \\displaystyle s = \\frac{H - 2 m}{N}, \\quad m \u0026lt; \\frac{H}{2} \u0026amp; \\footnotesize M = N, \\; W = H \\\\ \\\\ \\displaystyle s = \\frac{H-W}{M-N}, \\quad m = \\frac{M W - H N}{2(M - N)} \u0026amp; \\footnotesize N\u0026lt;M, \\; W\u0026lt;H \\quad \\textrm{or} \\quad N\u0026gt;M, \\; W\u0026gt;H \\end{cases}\\]  The first solution corresponds to the special case of a square page size. In this case, the grid must be square ($N=M$) and the margins may have an arbitrary value, with the cell size varying accordingly. This is not very surprising.\nThe second solution is where things become interesting. As intuition dictates, it is valid only if the grid orientation (portrait or landscape) matches the paper orientation. If so, uniform margins is achieved by choosing a cell size of $s = \\frac{H-W}{M-N}$.\nNote that the resulting margin $m$ may, depending on the parameters, be negative. In this case, the grid overflows all around the page by a constant distance. This making plotting/printing your piece inconvenient, you will have to adjust $N$ and/or $M$ to reach a positive margin value.\nThe demo I made a demonstration sketch made with vsketch. Here is how it looks:\n   This is a simplified version of the code which can be used as a starting point for your next grid-based design:\nimport itertools import vsketch class MySketch(vsketch.SketchClass): N = vsketch.Param(5, 1) M = vsketch.Param(7, 1) def draw(self, vsk: vsketch.Vsketch) -\u0026gt; None: vsk.size(\u0026#34;a4\u0026#34;, landscape=False, center=False) # disable auto-centering cell_size = (vsk.height - vsk.width) / (self.M - self.N) margin = (self.M * vsk.width - self.N * vsk.height) / 2 / (self.M - self.N) if cell_size \u0026gt; 0: # account for the computed margin vsk.translate(margin, margin) # draw the grid for i, j in itertools.product(range(self.N + 1), range(self.M + 1)): vsk.point(i * cell_size, j * cell_size) else: # ERROR: N and M values must be adjusted! pass ","permalink":"https://bylr.info/articles/2022/04/13/grid-layout/","summary":"The problem Several generative art algorithms, such as Truchet tiles, use a regular grid of square cells. For example, check this interactive demo from the Generative Design book or these few pieces of mine.\nNow, let\u0026rsquo;s say you want to generate an iteration of your algorithm for printing or plotting such that all margins around the grid are the same for the given paper size. You can of course adjust the number of cell rows and columns, but how should you size the cell such as to achieve uniform margins?","title":"How to scale a grid on a page for uniform margins?"},{"content":"I originally intended vpype 1.10 to be a \u0026lsquo;quick-and-dirty\u0026rsquo;, bug-fix-only release but it ended up being quite substantial, so let\u0026rsquo;s dive in.\nNew features and improvements   Improved support for layer pen width and opacity in the viewer (#448)\n The \u0026ldquo;Pen Width\u0026rdquo; and \u0026ldquo;Pen Opacity\u0026rdquo; menus are now named \u0026ldquo;Default Pen Width\u0026rdquo; and \u0026ldquo;Default Pen Opacity\u0026rdquo;. The layer opacity is now used for display by default. It can be overridden by the default pen opacity by checking the \u0026ldquo;Override\u0026rdquo; item from the \u0026ldquo;Default Pen Opacity\u0026rdquo; menu. The layer pen width is now used for display by default as well. Likewise, it can be overridden by checking the \u0026ldquo;Override\u0026rdquo; item from the \u0026ldquo;Default Pen Width\u0026rdquo; menu.     This alone is reason to upgrade, and, if we\u0026rsquo;re being honest, it should have been done in the previous release. The display logic of the viewer is now as follows:\n By default, honor the layer\u0026rsquo;s pen width and opacity if present. If pen width and/or opacity is not set, revert to the value set in the menu (0.3mm / 80% by default). Either or both of the displayed pen width and opacity can be forced to the value in the menu using the new \u0026ldquo;Override\u0026rdquo; menu item.  It is worth noting that opacity is not a standalone layer property, but is part of its RGBA color property (vp_color). Weirdly, vpype 1.9\u0026rsquo;s viewer would honor the base color (RGB), but not its alpha chanel. (See next feature, though.)\nBy the way, this article\u0026rsquo;s cover image is a screenshot of the viewer made with this command:\n(Yes, this is a properly syntax-highlighted vpype command made with a custom script and Rich\u0026rsquo;s new SVG export. This is very preliminary, but, in time, something to improve on and deploy more widely in the doc and elsewhere.)\n Added the alpha command to set layer opacity without changing the base color (#447, #451)   While working on the viewer improvements, I realized how inconvenient it was to set an arbitrary opacity value. Using CSS color names (e.g. color red) always sets opacity to 100% and there is no way around the hex notation for a custom value (e.g. color #ff00007f or color #f007). The new alpha command fills that gap: color red alpha 0.5.\n Added HPGL configuration for the Calcomp Artisan plotter (thanks to Andee Collard and @ithinkido) (#418)   Good news for Calcomp Artisan\u0026rsquo;s owners! Let this be a reminder that I welcome this kind of contribution. Though I\u0026rsquo;d like to, I can\u0026rsquo;t own every single type of vintage plotter! üòÖ\n  The read command now better handles SVGs with missing width or height attributes (#446)\nWhen the width or height attribute is missing or expressed as percent, the read command now attempts to use the viewBox attribute to set the page size, defaulting to 1000x1000px if missing. This behavior can be overridden with the --display-size and the --display-landscape parameters.\n   I recently came across a SVG with a viewBox defined but no width/height attributes:\n\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 1191.26 1684.49\u0026#34;\u0026gt;...\u0026lt;/svg\u0026gt; In such an instance, using the read command used to default to an A4 page size, while the vpype.read_svg() API (and friends) would default to a 1000x1000px page size. This is both inconsistent and missing the opportunity to fallback on the viewBox. vpype now fully delegates this fallback logic to svgelements, which does a good job at making the most of the available information. Also, if everything is missing (or width/height are expressed in percents), vpype consistently falls back to 1000x1000px.\n Added the --dont-set-date option to the write command (#442)   This one is a bit niche. vpype adds some metadata to the SVG, including the date and time at which it was generated (note the \u0026lt;dc:date\u0026gt; tag):\n$ vpype line 1cm 1cm 5cm 3cm layout a6 write -f svg - \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; ?\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; xmlns:cc=\u0026#34;http://creativecommons.org/ns\u0026#34; xmlns:dc=\u0026#34;http://purl.org/dc/elements/1.1/\u0026#34; xmlns:ev=\u0026#34;http://www.w3.org/2001/xml-events\u0026#34; xmlns:inkscape=\u0026#34;http://www.inkscape.org/namespaces/inkscape\u0026#34; xmlns:rdf=\u0026#34;http://www.w3.org/1999/02/22-rdf-syntax-ns\u0026#34; xmlns:sodipodi=\u0026#34;http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd\u0026#34; xmlns:xlink=\u0026#34;http://www.w3.org/1999/xlink\u0026#34; baseProfile=\u0026#34;tiny\u0026#34; height=\u0026#34;14.8cm\u0026#34; version=\u0026#34;1.2\u0026#34; viewBox=\u0026#34;0 0 396.85039370078744 559.3700787401575\u0026#34; width=\u0026#34;10.5cm\u0026#34;\u0026gt; \u0026lt;metadata\u0026gt; \u0026lt;rdf:RDF\u0026gt; \u0026lt;cc:Work\u0026gt; \u0026lt;dc:format\u0026gt;image/svg+xml\u0026lt;/dc:format\u0026gt; \u0026lt;dc:source\u0026gt;vpype line 1cm 1cm 5cm 3cm layout a6 write -f svg - \u0026lt;/dc:source\u0026gt; \u0026lt;dc:date\u0026gt;2022-04-07T10:15:00.532842\u0026lt;/dc:date\u0026gt; \u0026lt;/cc:Work\u0026gt; \u0026lt;/rdf:RDF\u0026gt; \u0026lt;/metadata\u0026gt; \u0026lt;defs/\u0026gt; \u0026lt;g fill=\u0026#34;none\u0026#34; id=\u0026#34;layer1\u0026#34; inkscape:groupmode=\u0026#34;layer\u0026#34; inkscape:label=\u0026#34;1\u0026#34; stroke=\u0026#34;#0000ff\u0026#34; style=\u0026#34;display:inline\u0026#34;\u0026gt; \u0026lt;line x1=\u0026#34;122.8346\u0026#34; x2=\u0026#34;274.0157\u0026#34; y1=\u0026#34;241.8898\u0026#34; y2=\u0026#34;317.4803\u0026#34;/\u0026gt; \u0026lt;/g\u0026gt; \u0026lt;/svg\u0026gt; This is all well and good until you automate the generation of SVGs under a version control system, which is what I did for vpype-perspective\u0026rsquo;s documentation figures. A PyDoIt dodo.py files converts any .vpy file it finds into corresponding SVGs ‚Äì basically a Python-powered, overcharged Makefile. (This is a rather neat process which, by the way, should get its own article someday.) In this kind of setup, having a ever-changing date in the SVG yields many unwanted VCS diffs which can now be avoided using this option.\nBug fixes  Fixed an issue with forlayer where the _n variable was improperly set (#443)   One word: inexcusable üôÑ\n Fixed an issue with write where layer opacity was included in the stroke attribute instead of using stroke-opacity, which, although compliant, was not compatible with Inkscape (#429)   This one is Inkscape\u0026rsquo;s fault. Using stroke-opacity is \u0026ldquo;more compatible\u0026rdquo; anyways, so it\u0026rsquo;s a good move regardless.\n Fixed an issue with vpype --help where commands from plug-ins would not be listed (#444)   I ran into an issue with Click where a sub-command plug-in using APIs from the top-level command\u0026rsquo;s package (a scheme widely used by vpype and its plug-ins) would fail because of circular imports. The workaround I used in vpype 1.9 meant that plug-ins were no longer listed in vpype --help. This is fixed now, but this may not be the end of the story. I tried ‚Äì and failed ‚Äì to reproduce the original issue in a minimal demo project and I\u0026rsquo;ll have to further dig into this someday.\n Fixed a minor issue where plug-ins would be reloaded each time vpype_cli.execute() is called (#444)   By \u0026ldquo;minor\u0026rdquo;, I mean that this amounted to a tiny performance hit for Python scripts using vpype\u0026rsquo;s execute() API multiple times.\n Fixed a rendering inconsistency in the viewer where the ruler width could vary by one pixel depending on the OpenGL driver/GPU/OS combination (#448)   The ruler of vpype\u0026rsquo;s viewer is supposed to be 20px wide, but it turns out that either of the horizontal or the vertical one was 21px wide instead. Which one? It depends on the platform and my Intel/AMD- and M1-based laptops disagreed on the matter! üò≤\nIt took me a while to discover that this is due to drawing lines at a 0.5px offset with respect to the pixel grid, leading to unpredictable rounding behavior. This is basically what happens when drawing horizontal or vertical lines with integer coordinates, and I wrote about it a few days ago.\nYou\u0026rsquo;d think that not one soul would care about this, but some of my tests are based on comparing newly rendered images of the viewer with previously-generated reference images, and those tests would fail on my new M1 Mac.\nAPI changes  Added vpype_cli.FloatType(), vpype_cli.IntRangeType(), vpype_cli.FloatRangeType(), and vpype_cli.ChoiceType() (#430, #447)   These Click types provide support for property and expression substitution. They were missing from vpype 1.9 because they aren\u0026rsquo;t needed internally. Plug-ins, however, wanted them, including flow imager and my new vpype-perspective.\n Changed vpype.Document.add_to_sources() to also modify the vp_source property (#431)   This will simplify the code needed to handle sources in plug-ins.\n Changed the parameter name of both vpype_viewer.Engine() and vpype_viewer.render_image() from pen_width and pen_opacity to default_pen_width and default_pen_opacity (breaking change) (#448) Added override_pen_width and override_pen_opacity boolean parameters to both vpype_viewer.Engine() and vpype_viewer.render_image() (#448) Added a set_date:bool = True argument to vpype.write_svg() (#442) Changed the default value of default_width and default_height arguments of vpype.read_svg() (and friends) to None to allow svgelement better handle missing width/height attributes (#446)   These are the API counterparts of some of the changes described before.\nOther changes  Added support for Python 3.10 and dropped support for Python 3.7 (#417)   Walruses have appeared already! ü¶≠\n Updated code base with modern typing syntax (using pyupgrade) (#427)   I was this year old when I learned that you can use modern typing syntax (such as list[int] | None) with older Python versions thanks to this statement:\nfrom __future__ import annotations I swiftly ran pyupgrade on the entire code base to bring it up to date.\n Updated the documentation template (#428)   It looks cleaner now IMO, though there is still a whole lot that could be improved.\n Updated installation instructions to use pipx (#428)   I have yet to get over how long it took me to realize this! üò≥ I\u0026rsquo;m sorry for everyone who has struggled with virtual environments to install vpype!\n","permalink":"https://bylr.info/articles/2022/04/07/annotated-release-notes-vpype-1.10/","summary":"I originally intended vpype 1.10 to be a \u0026lsquo;quick-and-dirty\u0026rsquo;, bug-fix-only release but it ended up being quite substantial, so let\u0026rsquo;s dive in.\nNew features and improvements   Improved support for layer pen width and opacity in the viewer (#448)\n The \u0026ldquo;Pen Width\u0026rdquo; and \u0026ldquo;Pen Opacity\u0026rdquo; menus are now named \u0026ldquo;Default Pen Width\u0026rdquo; and \u0026ldquo;Default Pen Opacity\u0026rdquo;. The layer opacity is now used for display by default. It can be overridden by the default pen opacity by checking the \u0026ldquo;Override\u0026rdquo; item from the \u0026ldquo;Default Pen Opacity\u0026rdquo; menu.","title":"Annotated Release Notes: vpype 1.10"},{"content":"When I started using my new M1 Max MacBook Pro in December, a bunch of vpype\u0026rsquo;s tests started to fail. The failing tests were all image-based: an image is rendered and then compared to a previously-generated, reference image. This process is made easy thanks to this Pytest fixture.\nIn this case, the reference images were generated long ago on my previous, Intel/AMD-based MacBook Pro. This GIF highlights the discrepancy I\u0026rsquo;d get with images generated on my new computer (notice how the ruler\u0026rsquo;s thickness varies):\nAs I\u0026rsquo;m currently working on this viewer again, I finally spent two days tracking this issue ‚Äì and finally found its cause.\nWithout giving it a thought, I first used integer coordinates for those ruler lines. However, coordinates refer to pixel boundaries ‚Äì not pixel centres. This means than an horizontal line with integer coordinates (e.g. [(2, 2), (7, 2)]) sits halfway between two consecutive rows of pixel:\nWhich of the 2nd or 3rd row of pixel eventually gets drawn is up to a coin toss ‚Äì or rather the rounding strategy of your particular OpenGL driver/GPU/OS combination.\nBy offsetting the coordinates by half a pixel (e.g. [(2, 2.5), (7, 2.5)]), one can force the line on a specific pixel row and avoid any rounding:\nThis makes the rendering more predictable across platforms.\nUltimately, the fix was very simple (I just changed the ruler thickness from 20 to 19.5), but figuring it out was tricky (relevant discussions on ModernGL\u0026rsquo;s Discord server). Hopefully I wont forget about it after writing this TIL.\n","permalink":"https://bylr.info/articles/2022/04/05/til-aligning-horizontal-or-vertical-lines-to-the-pixel-grid-with-opengl/","summary":"When I started using my new M1 Max MacBook Pro in December, a bunch of vpype\u0026rsquo;s tests started to fail. The failing tests were all image-based: an image is rendered and then compared to a previously-generated, reference image. This process is made easy thanks to this Pytest fixture.\nIn this case, the reference images were generated long ago on my previous, Intel/AMD-based MacBook Pro. This GIF highlights the discrepancy I\u0026rsquo;d get with images generated on my new computer (notice how the ruler\u0026rsquo;s thickness varies):","title":"TIL: aligning horizontal or vertical lines to the pixel grid with OpenGL"},{"content":"vpype \\ text -l1 -p 0 3.5cm \"Custom layer name/color/pen width\" \\ text -l2 -p 5cm 4.5cm -s 24 \"Properties\" \\ text -l2 -p 2cm 5.5cm -s 20 \"Expressions\" \\ text -l1 -p 6cm 6.5cm -s 24 \"Better/new block processors\" \\ text -l2 -p 3cm 8cm \"...and much more!\" \\ layout -m 0.3cm -l 10x3.5cm \\ penwidth -l2 0.5mm \\ color -l2 \"%Color(226,200,0)%\" \\ color -l1 \"%Color(3,118,207)%\" \\ color -l1 blue \\ show -- vpype 1.9 is finally out! üéâ\nI recently stumbled upon a post by Simon Willison where he promotes the idea of annotated release notes. As it turns out, this release is, by any metric I can think of, the biggest and most transformative so far. The associated change log is consequently rather unwieldy and calls, you guessed it üí°, for the present annotated release notes.\n(Note: although the original release notes are extensively quoted in this article, I reshuffled and shortened the original material. Make sure to check the base material for an authoritative list of change.)\nProperties Basics   Added support for global and per-layer properties (#359)\nThis feature introduces metadata to the pipeline in the form of properties which may either be attached to specific layers (layer property) or all of them (global property). Properties are identified by a name and may be of arbitrary type (e.g. integer, floating point, color, etc.). A number of system properties with a specific name (prefixed with vp_) and type are introduced to support some of the new features.\n   Metadata is data which says something about other data, and vpype lacked such a thing. Until now, what was passed from one command to the next consisted exclusively of paths sorted into layers, without any context such as what the color of these paths might be. One command could \u0026ldquo;know\u0026rdquo; about something (e.g. read knows, from the SVG, the color of a layer), but it could not \u0026ldquo;tell\u0026rdquo; the next command(s) about it.\nThis is no more, thanks to properties. They offer a generic mechanism to attach data to pipeline and layers. They are the backbone of several features introduced today, and lay the ground for future features within vpype or in plug-ins.\nLayer color, pen width, and name  Layer color, pen width, and name are now customizable (#359, #376, #389)  The read commands now sets layer color, pen width, and name based on the input SVG if possible. The new color, penwdith, and name commands can be used to modify layer color, pen width, and name. The new pens command can apply a predefined or custom scheme on multiple layers at once. Two common schemes are built-in: rgb and cmyk. Custom schemes can be defined in the configuration file. The show and write commands now take into account these layer properties.     Supporting arbitrary layer colors, pen widths, and names, has long been amongst the most requested features. Well, thanks to properties, here they are. It happens automagically when using read, and the new commands can further customise these values:\n$ vpype \\  rect --layer 1 0 0 5cm 5cm \\  color --layer 1 purple \\  penwidth --layer 1 0.5mm \\  circle --layer 2 4cm 4cm 2cm \\  color --layer 2 orange \\  penwidth --layer 2 5mm \\  show The new, high-level color, penwidth, and name commands are simple wrappers which change the value of specific system properties (i.e. vp_color, vp_pen_width, resp. vp_name):\n$ vpype random name \u0026#34;hello\u0026#34; color purple penwidth 0.1mm proplist --layer 1 listing 3 properties for layer 1 vp_color: (color) #800080 vp_name: (str) hello vp_pen_width: (float) 0.37795275590551186 System properties differ from \u0026ldquo;regular\u0026rdquo; properties only in the sense that they have special meaning to vpype. By convention, their name is prefixed with vp_.\nSpecial mention for the new pens command, which is short for here is the set of pens I intend to use for this plot. It sets in bulk layer colors, pen widths and/or names all at once, based on a built-in or custom configuration. For example, this produces a CMYK SVG using the flow imager plug-in:\n$ vpype \\  flow_img [...] --cmyk input.jpg \\  pens cmyk \\  write output.svg    Introduced new commands for low-level inspection and modification of properties (#359)\n propget: gets the value of a given global or layer property proplist: lists all global and/or layer properties and their value propset: sets the value of a given global or layer property propdel: deletes a given global or layer property propclear: removes all global and/or layer properties     These are low-level commands to interact with properties. Although they have limited use in real-world workflows, they come in handy when learning about properties or crafting complex pipelines.\n  Updated layer operation commands to handle properties (#359)\n When a single source layer is specified and --prob is not used, the lcopy and lmove commands now copy the source layer\u0026rsquo;s properties to the destination layer (possibly overwriting existing properties). When --prob is not used, the lswap command now swaps the layer properties as well. These behaviors can be disabled with the --no-prop option.     With properties, some of the layer manipulation commands became somewhat ambiguous. For example, what happens with properties when using lmove all 1 (merges all layers into layer one) or move --prob 0.5 1 2 (picks geometries from layer 1 with a 50% probability and moves them to layer 2)?\nI opted for a strategy where layer properties are affected only for unambiguous cases. This is basically when a single layer is moved/copied and when probabilistic behaviour is not used at all. In all other cases, the layer properties are left unchanged.\nFrom SVG attributes to properties   Added the --attr option to the read command to (optionally) sort geometries by attributes (e.g. stroke color, stroke width, etc.) instead of by SVG layer (#378, #389)\n  The read and write commands now preserve a sub-set of SVG attributes (experimental) (#359, #389)\nThe read command identifies SVG attributes (e.g. stroke-dasharray) which are common in all geometries within each layer. These attributes are saved as layer properties with their name prefixed with svg_ (e.g. svg_stroke-dasharray). The write command can optionally restore these attributes in the output SVG using the --restore-attribs option.\n   As noted, the read command now tries to extract SVG attributes and store them as layer properties. There are two motivations for that. First, it enables the write command to optionally restore these attributes in the output file, in order to achieve a higher degree of fidelity. (This feature is experimental and opt-in for the time being.) Second, it enables future features or plug-ins to do neat things such as generating hatch fills when fill is set to a color, or cutting paths in bits to emulate stroke-dasharray if defined.\nNow, since properties are only available at the layer level (or globally), read discards SVG attributes that are not shared amongst every paths within a given layer. Let\u0026rsquo;s take an example:\n\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; width=\u0026#34;650\u0026#34; height=\u0026#34;650\u0026#34;\u0026gt; \u0026lt;circle cx=\u0026#34;150\u0026#34; cy=\u0026#34;150\u0026#34; r=\u0026#34;100\u0026#34; stroke=\u0026#34;red\u0026#34; stroke-width=\u0026#34;0.5mm\u0026#34; fill=\u0026#34;green\u0026#34; /\u0026gt; \u0026lt;rect x=\u0026#34;400\u0026#34; y=\u0026#34;200\u0026#34; width=\u0026#34;200\u0026#34; height=\u0026#34;400\u0026#34; stroke=\u0026#34;blue\u0026#34; stroke-width=\u0026#34;0.5mm\u0026#34; fill=\u0026#34;green\u0026#34; /\u0026gt; \u0026lt;path d=\u0026#34;M250,600 l-200,0 l0,-200 z\u0026#34; stroke=\u0026#34;blue\u0026#34; stroke-width=\u0026#34;0.1mm\u0026#34; fill=\u0026#34;green\u0026#34; /\u0026gt; \u0026lt;/svg\u0026gt; This SVG only contains top-level elements, which vpype loads in layer 1 by default. The fill property is common to all paths and thus stored as a layer property, but the stroke and stroke-width attributes are heterogeneous and thus discarded. As a result, the show command uses the default color and pen width.\n$ vpype read example.svg proplist --layer 1 show listing 1 properties for layer 1 svg_fill: (str) green To address this issue, the read command has now the option to create layers based on SVG attributes instead of structure:\n$ vpype read --attr stroke --attr stroke-width example.svg proplist --layer all show listing 5 properties for layer 1 svg_fill: (str) green svg_stroke: (str) red svg_stroke-width: (str) 0.5mm vp_color: (color) #ff0000 vp_pen_width: (float) 1.8897648 listing 5 properties for layer 2 svg_fill: (str) green svg_stroke: (str) blue svg_stroke-width: (str) 0.5mm vp_color: (color) #0000ff vp_pen_width: (float) 1.8897648 listing 5 properties for layer 3 svg_fill: (str) green svg_stroke: (str) blue svg_stroke-width: (str) 0.1mm vp_color: (color) #0000ff vp_pen_width: (float) 0.37795296000000006 In this case, read creates one layer per unique combination of stroke and stroke-width attribute, resulting in a total of three layers, each assigned with the correct properties, and correctly displayed by show.\nSource files  The read command now records the source SVG paths in the vp_source and vp_sources system properties (see the documentation) (#397, #406, #408)   The idea of the vp_source and vp_sources properties is to keep track of the files from which the content of the pipeline originates from. The vp_source property is a single path, which is overwritten by the last command importing from a file. The vp_sources property is a set of all source files encountered so far. Both properties are pathlib.Path instances.\nThis is, for example, what happens when using read twice:\n$ vpype read machine_typography_01_3.svg read machine_typography_02_3.svg proplist -g listing 5 global properties svg_fill: (str) black svg_stroke: (str) none vp_page_size: (tuple) (396.850608, 559.3703808000001) vp_source: (PosixPath) /private/tmp/MT/machine_typography_02_3.svg vp_sources: (set) {PosixPath(\u0026#39;/private/tmp/MT/machine_typography_01_3.svg\u0026#39;), PosixPath(\u0026#39;/private/tmp/MT/machine_typography_02_3.svg\u0026#39;)} Here, vp_source points to the file read by the last read command, but vp_sources contains all two source files.\nCurrently, read is the only command which sets these variables, but the idea is that any command involved with reading a file (SVG or otherwise) should set these properties, including plug-ins such as hatched, flow imager, or vpype-embroidery.\nOne of the most common use case is to name the output file after the input file:\n$ vpype flow_img [...] my_image.png write \u0026#34;{vp_name.stem}_converted.svg\u0026#34; Note the use of a property substitution pattern, which brings us to the next topic.\nProperty substitution   Added property substitution to CLI user input (#395)\nThe input provided to most commands' arguments and options may now contain substitution patterns which will be replaced by the corresponding property value. Property substitution patterns are marked with curly braces (e.g. {property_name}) and support the same formatting capabilities as the Python\u0026rsquo;s format() function.\n   This is where things start becoming \u0026ldquo;meta\u0026rdquo;! ü§Ø\nAs shown in the previous example, the value of a property may now be used anywhere as input using property substitution patterns.\nHere is another example where the full path of the input file is drawn and displayed:\n$ vpype read example.svg text -p 0.5cm 0.5cm \u0026#34;{vp_source}\u0026#34; show Again, vp_source is a pathlib.Path instance, so {vp_source.name} (file name) or {vp_source.stem} (file name without extension) could be used instead.\nMultiple substitution patterns can be combined and mixed with regular text. For example, this creates an output file in the same directory as, and named after, the input file:\n$ vpype read example.svg linesort \\  write \u0026#34;{vp_source.parent}/{vp_source.stem}_optimised.svg\u0026#34; Of course, when using vpype interactively, it\u0026rsquo;s easier to simply spell out the output file name. Instead, this kind of mechanism makes it considerably easier to write generic, reusable shell scripts.\nNote that, since property substitution internally relies on Python\u0026rsquo;s str.format() function, the number formatting mini-language is available as well (e.g. {vp_pen_width:.02f}). See the documentation for some examples.\nNow, taking a step back, this feature is neat indeed, but its usefulness turns out to be limited in many non-trivial, real-world scenarios. I had hoped it would unlock several workflows I had in mind, but it just did not - or not elegantly enough. So much so that I even considered dropping the feature altogether.\nThis was a bit frustrating, to say the least. And ultimately lead to what is the next big chapter of this release.\nExpressions   Added expression substitution to CLI user input (#397)\nThe input provided to most command\u0026rsquo;s arguments and options may now contain expression patterns which are evaluated before the command is executed. Expression patterns are marked with the percent symbol % (e.g. %3+4%) and support a large subset of the Python language. A lot of examples were added in the cookbook.\n   This is possibly the most transformative feature brought to vpype since its inception: anything between pairs of % characters is now evaluated as (a sub-set of) Python code, and the result is substituted in the input before it reaches the actual command. The documentation has been updated with a whole new section about expressions (which I\u0026rsquo;m not going to repeat here), and the cookbook has plenty of examples making use of them. Do check them out for a taste of what expressions are capable of!\nThis feature blurs the lines between a mere CLI tool and a programming language. This begs the question of why not using a programming language in the first place, a point raised by fellow Python dev and flow imager author Jonas Serych. vpype even offers a proper API for that!\nHere are my thoughts about this:\n Users of vpype are often not Python developers \u0026ndash; or developers at all. Expressions build on existing vpype knowledge and bring, at least through examples and recipes that can be copy/pasted/customized, tiny bits of programs which are readily useful, without the need to learn much of the Python syntax and ecosystem. For many real-world cases (see the examples linked in the release notes), the resulting one-liners are more compact than the equivalent in proper code - even Python, even with vpype API. (Arbitrarily complex pipelines can of course be conceivably crafted as counter-examples, but this is besides the point.)   Added the eval command as placeholder for executing expressions (#397)   Though expressions can be used in any command\u0026rsquo;s input, some \u0026ldquo;space\u0026rdquo; dedicated to them in the pipeline can be useful. Typical cases include variable initialization or querying the user for some parameter with the input() function. Several examples shown or linked below make use of this.\nBlock processors   Improved block processors (#395, #397)\n Simplified and improved the infrastructure underlying block processors for better extensibility. The begin marker is now optional and implied whenever a block processor command is encountered. Note: the end marker must always be used to mark the end of a block. Commands inside the block now have access to the current layer structure and its metadata.     Block processors hardly got any love since the first release of vpype and, as far as I can tell, weren\u0026rsquo;t used much - if at all - due to their limitations. Properties and expressions completely reverse this situation and block processors are now where the magic happens. The changes above lay the ground work for this.\n  Improved the grid block processor (#397)\n The page size is now updated according to the grid size. The command now sets expression variables for use in the nested pipeline. Cells are now first iterated along rows instead of columns.    The repeat block processor now sets expression variables for use in the nested pipeline (#397)\n  Added forfile block processor to iterate over a list of file (#397)\n  Added forlayer block processor to iterate over the existing layers (#397)\n  The read command now will ignore a missing file if --no-fail parameter is used (#397)\n  Changed the initial default target layer to 1 (#395)\nPreviously, the first generator command of the pipeline would default to create a new layer if the --layer option was not provided. This could lead to unexpected behaviour in several situation. The target layer is now layer 1. For subsequent generators, the existing behaviour of using the previous generator target layer as default remains.\n   That\u0026rsquo;s two new block processor commands, and another two finally coming to life, plus a few changes to make them work better with real-world workflows.\nOne of the key changes is that block processors now set temporary expression variables (prefixed with _) that can be used in the nested pipeline. They are listed in each command\u0026rsquo;s help text:\n$ vpype grid --help Usage: vpype grid [OPTIONS] NX NY Creates a NX by NY grid of geometry [...] The following variables are set by `grid` and available for expressions: _nx: number of columns (NX) _ny: number of rows (NY) _n: total number of cells (NX*NY) _x: current column (0 to NX-1) _y: current row (0 to NY-1) _i: current cell (0 to _n-1) [...] Another novelty is the introduction of two new block processor commands:\n The forfile command accepts a pathname pattern (e.g. *.svg) and executes the nested pipeline for each of the paths it expends into. It makes things like batch processing files, merging multiple SVGs into a multilayer file, or laying out multiple files on a grid a breeze. The forlayer command executes the nested pipeline for each of the exising layers, which is useful, e.g., to export one file per layer.  Checks the related documentation for more details.\nThis example, taken from the grid layout recipe, demonstrates best what vpype 1.9 is about:\n$ vpype \\  eval \u0026#34;files=glob(\u0026#39;*.svg\u0026#39;)\u0026#34; \\  eval \u0026#34;cols=6; rows=ceil(len(files)/cols)\u0026#34; \\  eval \u0026#34;names={};n=100\u0026#34; \\  grid -o 10cm 15cm \u0026#34;%cols%\u0026#34; \u0026#34;%rows%\u0026#34; \\  read --no-fail \u0026#34;%files[_i] if _i \u0026lt; len(files) else \u0026#39;\u0026#39;%\u0026#34; \\  layout -m 0.5cm 10x15cm \\  forlayer \\  eval \u0026#34;%if _name not in names: names[_name] = n; n = n+1%\u0026#34; \\  lmove %_lid% \u0026#34;%names[_name]%\u0026#34; \\  end \\  end \\  write combined.svg It creates a grid layout from multiple SVG files, combining layers using their name (e.g. all \u0026ldquo;yellow\u0026rdquo; layers in input files are merged in a single \u0026ldquo;yellow\u0026rdquo; layer in the output file). Check the recipe for a detailed explanation.\nThis pipeline has it all:\n extensive use of expressions, two nested blocks, using their expression variables (prefixed with _), use of properties (via the _name variable set by forlayer, which contains the current layer\u0026rsquo;s vp_name property).  Here is how it looks when run on my Machine Typography #ptpx project:\nOther changes Note: This is the last version of vpype to support Python 3.7.\n It\u0026rsquo;s the year of the walrus for vpype! ü¶≠\n Added pagerotate command, to rotate the page layout (including geometries) by 90 degrees (#404)   This command is useful for plotters without native support for both portrait and landscape orientations. Your plotter support only, say, landscape orientation and you want to plot a portrait-oriented file? This pipeline does the trick:\n$ vpype read portrait_input.svg pagerotate wrote landscape_output.svg   Added --keep option to the ldelete command (to delete all layers but those specified) (#383)   There was formerly no way to delete all layers but one. The new --keep option fills this gap.\n Pinned poetry-core to 1.0.8 to enable editable installs (#410)   Poetry finally supports editable installs thanks to PEP 660 üéâ\nThis change is relevant when developing jointly on vpype and another project that depends on it (e.g. vsketch, or some plug-in). In such cases, vpype can now be installed in editable mode from a local checkout. Modifications made to it will immediately be available to the dependent project:\n$ cd my-plugin $ source venv/bin/activate $ git clone https://github.com/abey79/vpype ../vpype $ pip install -e ../vpype   Fixed an issue with the random command when using non-square area (#395)   That\u0026rsquo;s a two-year-old bug I can\u0026rsquo;t believe I hadn\u0026rsquo;t seen before üôÑ (I use random a lot when testing out stuff during development.)\n Renamed the bundled config file to vpype_config.toml (#359)   This is the config file bundled with vpype. It used to be called hpgl_devices.toml, but now it also contains the build-in configurations of the new pens command (cmyk and rgb). The old name didn\u0026rsquo;t make sense anymore.\nAPI changes:\n Moved all CLI-related APIs from vpype to vpype_cli (#388) Updated the block processor API (breaking change) (#395) \u0026hellip;   This release comes with scores of changes at the API level (to many to list here). Two of these changes deserve a note though.\nFirst, a fair amount of infrastructure used by the vpype CLI (e.g. the @generator decorator and friends) used to reside in the vpype package instead of vpype_cli. This is not ideal for many reasons and I\u0026rsquo;m moving away from it. vpype should be a \u0026ldquo;pure\u0026rdquo; library, whereas vpype_cli should contain everything needed for its CLI (and for plug-ins). These are not yet breaking changes but will generate deprecation warnings with most plug-ins. I will ensure that they are fixed ASAP.\nSecondly, as part of the block processor overhaul, the @block_processor decorator had breaking changes without backward-compatible deprecation. I am not aware of any third-party code actually using it, so this shouldn\u0026rsquo;t cause any issue.\nWhat\u0026rsquo;s next? Congrats if you got this far! üò≤üèÜ\nI hope you\u0026rsquo;ll enjoy vpype 1.9 as much as I sweated preparing it. üòÖ\nFeedback is welcome, via discussions for support/suggestions or issues for bugs. As always, I hang out on the drawingbots.net\u0026rsquo;s Discord server and am available for a chat.\nContributions are most welcome too, and the documentation is one area where help is always beneficial. I\u0026rsquo;ve gathered a few ideas of what can be done here.\nTo conclude, here are my probable areas of focus for the coming weeks/months:\n Given the scope of this release, I\u0026rsquo;m expecting to deal with increased user support and a few kirks and bugs to address. I\u0026rsquo;ll be available for this in the short term. Property-related stuff must be ported to vsketch, including a nice API to set layer name, color, pen-width, etc. The next big topic for vpype is its UX. At least, I want to improve the look and usability of the integrated help (using the brand new rich-click project), and add some visual feedback during execution. With support for Python 3.7 dropped, compatibility with Python 3.10 is now on the menu. Whatever user feedback might steer my attention to üòâ  ","permalink":"https://bylr.info/articles/2022/03/03/annotated-release-notes-vpype-1.9/","summary":"vpype \\ text -l1 -p 0 3.5cm \"Custom layer name/color/pen width\" \\ text -l2 -p 5cm 4.5cm -s 24 \"Properties\" \\ text -l2 -p 2cm 5.5cm -s 20 \"Expressions\" \\ text -l1 -p 6cm 6.5cm -s 24 \"Better/new block processors\" \\ text -l2 -p 3cm 8cm \"...and much more!\" \\ layout -m 0.3cm -l 10x3.5cm \\ penwidth -l2 0.5mm \\ color -l2 \"%Color(226,200,0)%\" \\ color -l1 \"%Color(3,118,207)%\" \\ color -l1 blue \\ show -- vpype 1.","title":"Annotated Release Notes: vpype 1.9"},{"content":"Hello again, world.\nLately, I\u0026rsquo;ve been meaning to document some of my personal projects and endeavours, and realised that I lacked a suitable outlet. I figured this would be a great occasion to test Hugo and GitHub Pages, so here we are!\nThis is what might ramble about in the future:\n Stuff about vpype, vsketch, and my other plotter-related software projects. Logs for some of my past and future projects (plotter-related or otherwise). Posts related to Home Assistant or other home-automation topics. Who knows what else.  Follow me on Twitter for updates.\n","permalink":"https://bylr.info/articles/2022/02/20/first-post/","summary":"\u003cp\u003eHello again, world.\u003c/p\u003e","title":"First post"},{"content":"My name is Antoine Beyeler. I am a multi-disciplinary engineer and entrepreneur with a PhD in flying robotics and interests in software, aviation, photography, generative art, plotters, and home automation. I live in Lausanne, Switzerland üá®üá≠.\nWith a few friends, I co-founded senseFly in 2009, which soon became the leader of fixed-wing mapping drones for surveyors, farmers, humanitarians and many others. Until my departure in 2018, I lead as CTO a team of up to 45 engineers and developed multiple generations of end-to-end mapping drone systems. I am now involved in advisory capacity with several start-ups in the Geneva region.\nLike my skills, my contributions to generative art lean on the technical rather than the artistic side üë®‚Äçüíª. Among other things, I created and maintain two open-source projects for plotter generative artists:\n vpype: a Swiss-Army-knife CLI to generate, manage, process, and convert vector graphic files for plotters, vsketch: a Python plotter generative art framework and workflow automation tool.  When I am not at the computer, you may find me piloting a helicopter üöÅ or behind the camera üì∑ on my travels.\nAbout this site This site is built using Hugo and the PaperMod theme. It is hosted on GitHub Pages and deployed using a simple GitHub Actions workflow. Privacy-friendly and GDPR-compliant analytics services are provided by Plausible (a paid service, so neither you or I are the product).\n","permalink":"https://bylr.info/about/","summary":"My name is Antoine Beyeler. I am a multi-disciplinary engineer and entrepreneur with a PhD in flying robotics and interests in software, aviation, photography, generative art, plotters, and home automation. I live in Lausanne, Switzerland üá®üá≠.\nWith a few friends, I co-founded senseFly in 2009, which soon became the leader of fixed-wing mapping drones for surveyors, farmers, humanitarians and many others. Until my departure in 2018, I lead as CTO a team of up to 45 engineers and developed multiple generations of end-to-end mapping drone systems.","title":"About"}]